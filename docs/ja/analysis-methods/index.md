# 分析手法ドキュメント

ラマン分光分析アプリケーションで利用可能なすべての分析手法の完全なリファレンス

---

## 📚 概要

このセクションでは、アプリケーションで実装されているすべての分析手法の包括的なドキュメントを提供します。各手法には、理論的背景、パラメータの詳細、使用例、およびベストプラクティスが含まれています。

---

## 🎯 手法カテゴリ

### 1. 前処理手法（40以上）

**目的**: スペクトルデータの品質を改善し、分析に適した形式に標準化

**カテゴリ**:
- **ベースライン補正** (8手法) - ベースラインドリフトの除去
- **スムージングとノイズ除去** (6手法) - ランダムノイズの低減
- **正規化** (7手法) - スケールの標準化
- **微分** (4手法) - ピーク分解能の向上
- **高度な手法** (15手法) - 専門的な前処理

[詳細を見る →](preprocessing.md)

### 2. 探索的分析（6手法）

**目的**: データ構造とパターンの発見

**手法**:
- **PCA** - 線形次元削減
- **UMAP** - 非線形マニフォールド学習
- **t-SNE** - 視覚化に最適化された次元削減
- **K-means** - パーティション型クラスタリング
- **階層的クラスタリング** - ツリーベースのグルーピング
- **DBSCAN** - 密度ベースのクラスタリング

[詳細を見る →](exploratory.md)

### 3. 統計分析（8カテゴリ）

**目的**: グループ間の差異を統計的に検定

**カテゴリ**:
- **パラメトリック検定** - t検定、ANOVA
- **ノンパラメトリック検定** - Mann-Whitney U、Wilcoxon
- **多重比較補正** - Bonferroni、FDR
- **相関分析** - Pearson、Spearman
- **効果量** - Cohen's d、η²
- **信頼区間** - ブートストラップ法
- **多変量検定** - MANOVA、PERMANOVA
- **生存分析** - Kaplan-Meier、Cox回帰

[詳細を見る →](statistical.md)

### 4. 機械学習（4アルゴリズム）

**目的**: 予測モデルの構築と分類

**アルゴリズム**:
- **SVM** - サポートベクターマシン
- **Random Forest** - アンサンブル決定木
- **XGBoost** - 勾配ブースティング
- **ロジスティック回帰** - 確率的分類

**注記**: ニューラルネットワーク（MLP）は将来のリリースで予定されています。

[詳細を見る →](machine-learning.md)

---

## 🔍 手法選択ガイド

### 目的別推奨手法

#### データの探索

**目的**: データの全体像を把握したい

**推奨フロー**:
```
1. 前処理: AsLS + ベクトルノルム
2. 分析: PCA (2-3成分)
3. 可視化: スコアプロット
4. 解釈: ローディング確認
```

**理由**: PCAは高速で解釈しやすく、最初のデータ探索に最適

#### グループ分離の可視化

**目的**: グループ間の違いを視覚的に確認したい

**推奨フロー**:
```
1. 前処理: AsLS + SNV
2. 分析: UMAP (n_neighbors=15, min_dist=0.1)
3. 可視化: 色分けされたスコアプロット
4. 補完: K-meansクラスタリング
```

**理由**: UMAPは非線形構造を捉え、グループ分離を強調

#### 統計的差異検定

**目的**: グループ間の差が統計的に有意か確認したい

**2グループの場合**:
```
1. 前処理: AsLS + Savitzky-Golay + 正規化
2. 検定: 
   - 正規分布の場合: t検定
   - 非正規分布の場合: Mann-Whitney U検定
3. 効果量: Cohen's d
4. 可視化: ボックスプロット
```

**3グループ以上の場合**:
```
1. 前処理: 同上
2. 検定:
   - 一元配置ANOVA（注：現在UIで無効）
   - または各ペアでMann-Whitney U検定
3. 事後検定: Tukey HSD（参考用）
4. 多重比較補正: Bonferroni またはFDR
```

#### 予測モデルの構築

**目的**: 新しいサンプルを分類する予測モデルを作りたい

**バランスの取れたデータの場合**:
```
1. 前処理: 最適化されたパイプライン
2. データ分割: 70% train, 30% test
3. アルゴリズム: Random Forest (n_estimators=100)
4. クロスバリデーション: 5-fold
5. 評価: 精度、F1スコア、混同行列
```

**不均衡データの場合**:
```
1. 前処理: 同上
2. サンプリング: SMOTEまたは重み調整
3. アルゴリズム: XGBoost (scale_pos_weight調整)
4. 評価: ROC-AUC、精密度-再現率曲線
```

---

## 📊 比較表

### 次元削減手法の比較

| 手法      | 線形/非線形 | 速度       | 解釈性 | 最適な用途           |
| --------- | ----------- | ---------- | ------ | -------------------- |
| **PCA**   | 線形        | 非常に高速 | 高い   | 初期探索、特徴量削減 |
| **UMAP**  | 非線形      | 高速       | 中程度 | 複雑な構造の可視化   |
| **t-SNE** | 非線形      | 低速       | 低い   | 最終的な可視化       |

**選択ガイド**:
- **PCA**: 最初に試す、高速、解釈可能
- **UMAP**: 非線形構造がある場合、グループ分離を強調
- **t-SNE**: 最終的な出版用図、美しい可視化

### クラスタリング手法の比較

| 手法        | クラスター形状 | 速度 | パラメータ             | 最適な用途                       |
| ----------- | -------------- | ---- | ---------------------- | -------------------------------- |
| **K-means** | 球形           | 高速 | 少ない (k)             | 明確に分離されたグループ         |
| **階層的**  | 任意           | 低速 | 少ない (linkage)       | 階層構造の探索                   |
| **DBSCAN**  | 任意           | 中速 | 2つ (eps, min_samples) | 密度ベースのグループ、外れ値除去 |

**選択ガイド**:
- **K-means**: 高速、クラスター数が既知
- **階層的**: デンドログラムで構造を可視化
- **DBSCAN**: 不規則な形状、外れ値を自動検出

### 機械学習アルゴリズムの比較

| アルゴリズム           | 速度       | 精度       | 解釈性     | 過学習リスク | 最適な用途                 |
| ---------------------- | ---------- | ---------- | ---------- | ------------ | -------------------------- |
| **ロジスティック回帰** | 非常に高速 | 中程度     | 非常に高い | 低い         | 線形分離可能、ベースライン |
| **SVM**                | 中速       | 高い       | 低い       | 中程度       | 高次元データ、少数サンプル |
| **Random Forest**      | 中速       | 非常に高い | 中程度     | 低い         | 一般的な用途、堅牢         |
| **XGBoost**            | 低速       | 非常に高い | 低い       | 中程度       | 最高精度が必要、競技       |

**注記**: ニューラルネットワーク（MLP）は将来のリリースで予定されています。

**選択ガイド**:
- **初心者**: Random Forest（バランスが良い）
- **高精度必要**: XGBoost（調整が重要）
- **解釈性必要**: ロジスティック回帰（シンプル）
- **小規模データ**: SVM（過学習に強い）
- **大規模データ**: XGBoost または Random Forest（スケーラブル）

---

## 🎓 理論的背景

### なぜ前処理が重要か

**生スペクトルの問題**:

1. **ベースラインドリフト**
   - 原因: 蛍光、散乱、検出器応答
   - 影響: ピーク強度の歪み、比較困難
   - 解決: AsLS、AirPLS

2. **ノイズ**
   - 原因: 検出器、光源、環境
   - 影響: 信号品質の低下、誤検出
   - 解決: Savitzky-Golay、ガウシアンフィルタ

3. **スケール変動**
   - 原因: 濃度、測定条件、サンプル準備
   - 影響: 定量的比較が不可能
   - 解決: 正規化（ベクトルノルム、SNV）

4. **ピーク重複**
   - 原因: 複雑な混合物、広いピーク
   - 影響: 定量化困難、混合物解析難
   - 解決: 微分、ピークデコンボリューション

**前処理の効果**:
```
生スペクトル
  S/N比: 10
  ベースライン: 傾斜
  ピーク: 不明瞭
      ↓
前処理パイプライン
  1. AsLS → ベースライン平坦化
  2. Savitzky-Golay → ノイズ50%減
  3. ベクトルノルム → スケール統一
      ↓
処理済みスペクトル
  S/N比: 50
  ベースライン: 平坦
  ピーク: 明瞭
```

### 次元削減の数学

**PCAの原理**:

主成分分析は、データの分散を最大化する方向（主成分）を見つけます。

```
データ行列 X (n × p)
  n: サンプル数
  p: 波数（特徴量数）

1. 中心化: X_centered = X - mean(X)
2. 共分散行列: C = (X_centered)^T × X_centered
3. 固有値分解: C = V × Λ × V^T
4. スコア: T = X_centered × V
5. 説明分散: explained_variance_ratio = λ_i / Σλ_j
```

**主成分の解釈**:
- **スコア (T)**: サンプルの新しい座標
- **ローディング (V)**: 各波数の寄与
- **説明分散**: 各成分が捉える情報量

### 統計検定の理論

**仮説検定のフレームワーク**:

```
1. 仮説設定:
   H0 (帰無仮説): グループ間に差がない
   H1 (対立仮説): グループ間に差がある

2. 有意水準の設定:
   α = 0.05 (5%の偽陽性を許容)

3. 検定統計量の計算:
   t検定の場合: t = (μ1 - μ2) / SE

4. p値の計算:
   p値 = P(観測データ以上に極端なデータ | H0が真)

5. 判定:
   p < α → H0を棄却、差がある
   p ≥ α → H0を棄却できない、差があるとは言えない
```

**多重比較問題**:

複数の検定を行うと、偽陽性が増加：

```
検定数: n = 100
有意水準: α = 0.05

期待される偽陽性数:
  n × α = 100 × 0.05 = 5

解決策:
1. Bonferroni補正: α_adjusted = α / n = 0.0005
2. FDR補正: q値を計算、より緩やか
```

### 機械学習の基礎理論

**学習パラダイム**:

```
教師あり学習:
  入力: X (特徴量) + y (ラベル)
  目的: f(X) = y を学習
  例: 分類、回帰

教師なし学習:
  入力: X (特徴量のみ)
  目的: データ構造を発見
  例: クラスタリング、次元削減
```

**バイアス-バリアンストレードオフ**:

```
総誤差 = バイアス² + バリアンス + ノイズ

バイアス (Bias):
  - モデルの単純化による誤差
  - 高い → 過小適合 (underfitting)

バリアンス (Variance):
  - 訓練データへの感度
  - 高い → 過学習 (overfitting)

最適モデル:
  - バイアスとバリアンスのバランス
  - クロスバリデーションで選択
```

**正則化**:

過学習を防ぐ技術：

```
L1正則化 (Lasso):
  Loss = MSE + λ Σ|w_i|
  効果: 特徴量選択、疎なモデル

L2正則化 (Ridge):
  Loss = MSE + λ Σw_i²
  効果: 重みの縮小、安定化

Elastic Net:
  Loss = MSE + λ₁Σ|w_i| + λ₂Σw_i²
  効果: L1とL2の組み合わせ
```

---

## 💡 実践的なヒント

### 前処理パイプラインの構築

**一般的なパイプライン**:

```
目的: 探索的分析
  1. AsLS (lambda=100000, p=0.01)
  2. Savitzky-Golay (window=11, polyorder=3)
  3. ベクトルノルム

目的: 定量分析
  1. AsLS (lambda=100000, p=0.01)
  2. SNV
  3. MSC

目的: 微小な差の検出
  1. AirPLS (lambda=100, porder=1)
  2. 一次微分 (Savitzky-Golay, deriv=1)
  3. 分位点正規化 (q=0.9)
```

**パラメータ調整のコツ**:

```
ベースライン補正 (AsLS):
  lambda:
    - 大きい値 (10⁶): 滑らかなベースライン
    - 小さい値 (10³): ピークに追従
  p:
    - 小さい値 (0.001): ピークを保護
    - 大きい値 (0.1): より強い補正

スムージング (Savitzky-Golay):
  window:
    - 小さい値 (5-7): ピーク保持
    - 大きい値 (15-21): ノイズ除去
  polyorder:
    - 通常 2-3 を使用
    - window より小さい必要
```

### 分析結果の解釈

**PCAスコアプロット**:

```
良い結果:
  ✓ グループが明確に分離
  ✓ グループ内の分散が小さい
  ✓ 説明分散 > 70%

問題がある結果:
  ✗ グループが重なる → 前処理を改善
  ✗ 外れ値が多い → データ品質を確認
  ✗ 説明分散 < 50% → より多くの成分が必要
```

**混同行列の読み方**:

```
実際のクラス
        A    B
予測  A [90   5]
      B [10  95]

精度 (Accuracy): (90+95)/200 = 92.5%
精密度 (Precision) クラスA: 90/(90+10) = 90%
再現率 (Recall) クラスA: 90/(90+5) = 94.7%
F1スコア クラスA: 2×(90×94.7)/(90+94.7) = 92.3%
```

### 一般的な落とし穴

**前処理**:
```
❌ 避けるべき:
  - 過度なスムージング → ピーク情報の損失
  - 不適切な正規化 → スケールの歪み
  - 順序の誤り → 効果が相殺

✅ ベストプラクティス:
  - プレビューで各ステップを確認
  - パラメータを段階的に調整
  - 複数のサンプルでテスト
```

**分析**:
```
❌ 避けるべき:
  - 単一の手法に依存
  - 前処理なしの分析
  - 視覚的検証の欠如

✅ ベストプラクティス:
  - 複数の手法で確認
  - 適切な前処理を適用
  - 統計的検証を実施
```

**機械学習**:
```
❌ 避けるべき:
  - データ分割なし → 過学習
  - クロスバリデーションなし → 汎化性能不明
  - 単一メトリクス → 偏った評価

✅ ベストプラクティス:
  - 適切なデータ分割
  - 5-10 fold クロスバリデーション
  - 複数のメトリクスで評価
```

---

## 📚 詳細ドキュメント

各カテゴリの詳細なドキュメントは、以下のページをご覧ください：

- **[前処理手法](preprocessing.md)** - 40以上の前処理アルゴリズムの完全なリファレンス
- **[探索的分析](exploratory.md)** - PCA、UMAP、クラスタリングの詳細
- **[統計分析](statistical.md)** - 統計検定と相関分析
- **[機械学習](machine-learning.md)** - 分類アルゴリズムとモデル評価

---

## 🔗 関連リソース

- **[ユーザーガイド](../user-guide/index.md)** - アプリケーションの使用方法
- **[はじめに](../getting-started.md)** - セットアップと基本の流れ
- **[API リファレンス](../api/index.md)** - 開発者向けドキュメント

---

**最終更新**: 2026年1月24日 | **バージョン**: 1.0.0

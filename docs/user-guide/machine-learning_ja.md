# 機械学習ガイド

ラマンスペクトルデータを用いた機械学習モデルの構築

---

## 📋 目次

- [機械学習の基礎](#機械学習の基礎)
- [データ準備](#データ準備)
- [アルゴリズム選択](#アルゴリズム選択)
- [モデルトレーニング](#モデルトレーニング)
- [モデル評価](#モデル評価)
- [ハイパーパラメータ調整](#ハイパーパラメータ調整)
- [特徴量選択](#特徴量選択)
- [モデルの解釈](#モデルの解釈)
- [予測と実運用](#予測と実運用)

---

## 機械学習の基礎

### 機械学習とは

```
定義:
データからパターンを学習し、
新しいデータに対して予測を行う手法

ラマン分光での応用:
- 化合物の識別・分類
- 品質管理
- 濃度予測
- 異常検出
- 偽造品検出
```

### 学習パラダイム

#### 教師あり学習

```
ラベル（正解）付きデータから学習

分類:
目的: カテゴリを予測
例: 
- サンプルA、B、Cの識別
- 合格/不合格の判定
- 正常/異常の検出

回帰:
目的: 連続値を予測
例:
- 濃度の予測
- 純度の推定
```

#### 教師なし学習

```
ラベルなしデータから構造を発見

クラスタリング:
- グループの自動発見
- 外れ値検出

次元削減:
- PCA、UMAP（前処理や可視化に使用）
```

### ワークフロー

```
1. データ準備
   ├── データ分割（Train/Test）
   ├── 前処理
   └── 特徴量エンジニアリング

2. モデル選択
   └── アルゴリズムの選択

3. トレーニング
   ├── モデルの学習
   └── ハイパーパラメータ調整

4. 評価
   ├── 精度メトリクス
   ├── 混同行列
   └── クロスバリデーション

5. 予測
   └── 新しいデータへの適用
```

---

## データ準備

### データ分割

#### Train/Test分割

```
機械学習タブ → 設定 → データ分割

推奨分割:
- トレーニング: 70%
- テスト: 30%

または
- トレーニング: 60%
- 検証: 20%
- テスト: 20%

重要: ランダムシードを設定
random_seed: 42（再現性のため）
```

#### 層化分割（Stratified Split）

```
設定 → データ分割:
□ 層化分割を使用

効果:
各クラスの比率を維持

例:
全データ:
- クラスA: 60サンプル（60%）
- クラスB: 40サンプル（40%）

トレーニングセット（70サンプル）:
- クラスA: 42サンプル（60%）
- クラスB: 28サンプル（40%）

テストセット（30サンプル）:
- クラスA: 18サンプル（60%）
- クラスB: 12サンプル（40%）
```

### クロスバリデーション

```
設定 → クロスバリデーション:
□ CVを有効化

パラメータ:
n_folds: 5（推奨）

仕組み:
データを5つに分割
各分割を1回ずつテストセットとして使用

評価:
5回の平均精度 ± 標準偏差

利点:
- より信頼性の高い評価
- すべてのデータを使用
- 過学習の検出

例:
Fold 1: 85%
Fold 2: 88%
Fold 3: 86%
Fold 4: 87%
Fold 5: 84%

平均: 86% ± 1.5%
→ 安定したモデル
```

### 前処理パイプライン

```
機械学習の前に必須の前処理:

1. ベースライン補正
   AsLS（lambda=100000, p=0.01）

2. スムージング
   Savitzky-Golay（window=11, polyorder=3）

3. 正規化
   ベクトルノルム または SNV

（オプション）
4. 次元削減
   PCA（n_components=50-100）

5. 微分
   一次微分（特徴を強調したい場合）
```

---

## アルゴリズム選択

### Random Forest

#### 特徴

```
決定木の集合（アンサンブル）

利点:
✓ 高精度
✓ 過学習に強い
✓ 特徴量の重要度を提供
✓ ハイパーパラメータ調整が比較的簡単
✓ 初心者に優しい

欠点:
✗ 解釈性が低い（ブラックボックス）
✗ モデルサイズが大きい
```

#### 主要パラメータ

```
n_estimators: 木の数
  - デフォルト: 100
  - 推奨: 100-500
  - 多いほど精度向上（計算時間とトレードオフ）

max_depth: 木の最大深さ
  - デフォルト: None（無制限）
  - 過学習の場合: 5-20に制限

min_samples_split: 分割に必要な最小サンプル数
  - デフォルト: 2
  - 過学習の場合: 5-10に増やす

min_samples_leaf: 葉の最小サンプル数
  - デフォルト: 1
  - 過学習の場合: 2-5に増やす

max_features: 各分割で考慮する特徴量数
  - 'sqrt': √(n_features)（推奨）
  - 'log2': log₂(n_features)
  - None: すべての特徴量
```

#### 使用例

```
機械学習タブ → アルゴリズム → Random Forest

基本設定:
n_estimators: 200
max_depth: None
min_samples_split: 2
max_features: 'sqrt'

→ トレーニング

期待される結果:
- トレーニング精度: 90-95%
- テスト精度: 85-90%
- 計算時間: 10-60秒
```

### SVM（Support Vector Machine）

#### 特徴

```
利点:
✓ 高次元データに強い
✓ 少数のサンプルでも動作
✓ 理論的基盤が強固
✓ カーネルトリックで非線形境界

欠点:
✗ 大規模データでは遅い
✗ ハイパーパラメータ調整が重要
✗ 確率推定が直接的でない
```

#### 主要パラメータ

```
C: 正則化パラメータ
  - 小さい（0.1）: 単純なモデル、汎化性高い
  - 大きい（10）: 複雑なモデル、過学習リスク
  - デフォルト: 1.0

kernel: カーネル関数
  - 'linear': 線形分離可能な場合
  - 'rbf': 非線形（デフォルト、推奨）
  - 'poly': 多項式
  - 'sigmoid': シグモイド

gamma: RBFカーネルのパラメータ
  - 'scale': 1 / (n_features × X.var())（推奨）
  - 'auto': 1 / n_features
  - 小さい: 滑らかな決定境界
  - 大きい: 複雑な決定境界
```

#### 使用例

**線形SVM（高速、解釈しやすい）**
```
機械学習タブ → SVM → Linear

設定:
C: 1.0
kernel: 'linear'

適用ケース:
- 線形分離可能なデータ
- 特徴量の重要度を知りたい
- 高速処理が必要
```

**RBF SVM（非線形、高精度）**
```
機械学習タブ → SVM → RBF

設定:
C: 1.0
kernel: 'rbf'
gamma: 'scale'

適用ケース:
- 非線形な決定境界
- 高精度が必要
- サンプル数が中程度（< 10,000）
```

### XGBoost

#### 特徴

```
勾配ブースティング決定木

利点:
✓ 最高の精度（多くの場合）
✓ 欠損値の処理
✓ 正則化機能
✓ 早期停止

欠点:
✗ ハイパーパラメータが多い
✗ 調整に時間がかかる
✗ やや遅い
```

#### 主要パラメータ

```
n_estimators: 木の数
  - 推奨: 100-300
  - 早期停止と組み合わせる

max_depth: 木の深さ
  - デフォルト: 6
  - 推奨: 3-10
  - 深すぎると過学習

learning_rate（eta）: 学習率
  - デフォルト: 0.3
  - 推奨: 0.01-0.3
  - 小さいほど汎化性能向上（n_estimators増やす）

subsample: サンプルのサブサンプリング比率
  - デフォルト: 1.0
  - 推奨: 0.8-1.0
  - 過学習防止

colsample_bytree: 特徴量のサブサンプリング
  - デフォルト: 1.0
  - 推奨: 0.8-1.0

reg_lambda: L2正則化
  - デフォルト: 1.0
  - 過学習の場合: 増やす

reg_alpha: L1正則化
  - デフォルト: 0
  - スパース解が必要な場合: 増やす
```

#### 使用例

**基本設定**
```
機械学習タブ → XGBoost

設定:
n_estimators: 200
max_depth: 6
learning_rate: 0.1
subsample: 0.8
colsample_bytree: 0.8

→ トレーニング

期待される結果:
- 最高の精度
- 計算時間: 1-10分
```

**高速設定**
```
tree_method: 'hist'
n_estimators: 100
max_depth: 3
learning_rate: 0.3

用途: 迅速な探索
```

**高精度設定**
```
n_estimators: 500
max_depth: 8
learning_rate: 0.01
subsample: 0.8
colsample_bytree: 0.8
reg_lambda: 2.0
early_stopping_rounds: 20

用途: 最終モデル
```

### ロジスティック回帰

#### 特徴

```
線形モデル

利点:
✓ 非常に高速
✓ 解釈しやすい（係数の意味）
✓ 確率出力
✓ 過学習しにくい

欠点:
✗ 線形分離のみ
✗ 複雑なパターンは捉えられない
```

#### パラメータ

```
C: 正則化の強さの逆数
  - デフォルト: 1.0
  - 小さい: 強い正則化（単純なモデル）
  - 大きい: 弱い正則化（複雑なモデル）

penalty: 正則化タイプ
  - 'l2'（Ridge）: デフォルト、推奨
  - 'l1'（Lasso）: スパース解
  - 'elasticnet': L1 + L2

solver: 最適化アルゴリズム
  - 'lbfgs': デフォルト、小〜中規模データ
  - 'liblinear': 小規模データ
  - 'saga': 大規模データ、elasticnet対応
```

#### 使用例

```
機械学習タブ → ロジスティック回帰

設定:
C: 1.0
penalty: 'l2'
solver: 'lbfgs'
max_iter: 100

適用ケース:
- ベースラインモデル
- 高速処理が必要
- モデルの解釈が重要
- 線形分離可能なデータ
```

### ニューラルネットワーク（MLP）

#### 特徴

```
多層パーセプトロン

利点:
✓ 複雑な非線形パターン
✓ 大規模データに適する
✓ 転移学習可能

欠点:
✗ ハイパーパラメータが多い
✗ 訓練に時間がかかる
✗ 解釈が困難
✗ 小規模データでは過学習しやすい
```

#### パラメータ

```
hidden_layer_sizes: 隠れ層の構造
  - (100,): 1層100ユニット
  - (100, 50): 2層（100→50ユニット）
  - 推奨: (50,) または (100, 50)

activation: 活性化関数
  - 'relu': デフォルト、推奨
  - 'tanh': 時々有用
  - 'logistic': シグモイド

alpha: L2正則化
  - デフォルト: 0.0001
  - 過学習の場合: 0.001-0.01に増やす

learning_rate_init: 初期学習率
  - デフォルト: 0.001
  - 推奨: 0.001-0.01

max_iter: 最大エポック数
  - デフォルト: 200
  - 推奨: 500-1000（早期停止と併用）

early_stopping: 早期停止
  - True: 検証誤差が改善しなくなったら停止
```

#### 使用例

```
機械学習タブ → ニューラルネットワーク

基本設定:
hidden_layer_sizes: (100, 50)
activation: 'relu'
alpha: 0.0001
learning_rate_init: 0.001
max_iter: 500
early_stopping: True

適用ケース:
- 大規模データ（> 1000サンプル）
- 複雑な非線形関係
- 最高の精度が必要
```

---

## モデルトレーニング

### 基本的なトレーニング

```
機械学習タブ → トレーニング:

ステップ:
1. アルゴリズムを選択
2. パラメータを設定
3. 「トレーニング開始」をクリック

進行状況:
プログレスバーとログが表示

完了後:
- トレーニング精度
- テスト精度
- 計算時間
- モデルの保存
```

### グリッドサーチ

```
設定 → ハイパーパラメータ調整 → グリッドサーチ

定義:
複数のパラメータ組み合わせを網羅的に探索

例（Random Forest）:
n_estimators: [100, 200, 500]
max_depth: [None, 10, 20]
min_samples_split: [2, 5, 10]

→ 3 × 3 × 3 = 27 組み合わせを評価

各組み合わせでクロスバリデーション

最良の組み合わせを自動選択

注意:
- 計算時間: CV折数 × 組み合わせ数
- 5-fold CV × 27組み合わせ = 135回のトレーニング
```

#### 効率的なグリッドサーチ

```
探索範囲を絞る:

粗い探索:
n_estimators: [100, 500]
max_depth: [5, 20]

結果: max_depth=5が良い

細かい探索:
max_depth: [3, 5, 7, 10]

段階的に絞り込む
```

### ランダムサーチ

```
設定 → ハイパーパラメータ調整 → ランダムサーチ

グリッドサーチとの違い:
- ランダムに組み合わせをサンプリング
- 指定した回数だけ評価

利点:
- より効率的
- 広い範囲を探索可能
- 計算時間を制御しやすい

例:
n_iter: 30（30組み合わせを試す）
→ 5-fold CV × 30 = 150回のトレーニング
```

---

## モデル評価

### 分類メトリクス

#### 精度（Accuracy）

```
定義:
正しく分類されたサンプルの割合

Accuracy = (TP + TN) / (TP + TN + FP + FN)

TP: 真陽性
TN: 真陰性
FP: 偽陽性
FN: 偽陰性

解釈:
90%精度 = 100サンプル中90が正解

注意:
不均衡データでは誤解を招く

例:
99サンプルがクラスA、1サンプルがクラスB
すべてクラスAと予測 → 精度99%
（実際はクラスBを全く検出できていない）
```

#### 適合率（Precision）

```
定義:
陽性と予測したもののうち、実際に陽性だった割合

Precision = TP / (TP + FP)

解釈:
- 高い: 陽性予測が信頼できる
- 偽陽性が少ない

用途:
偽陽性のコストが高い場合
例: 疾病診断（誤って陽性と診断するコスト）
```

#### 再現率（Recall / Sensitivity）

```
定義:
実際の陽性のうち、正しく検出できた割合

Recall = TP / (TP + FN)

解釈:
- 高い: 陽性をよく検出できる
- 偽陰性が少ない

用途:
偽陰性のコストが高い場合
例: がん検診（見逃すことのコスト）
```

#### F1スコア

```
定義:
適合率と再現率の調和平均

F1 = 2 × (Precision × Recall) / (Precision + Recall)

解釈:
- PrecisionとRecallのバランス
- 不均衡データでも有用

目標:
F1 > 0.8: 良好
F1 > 0.9: 優秀
```

### 混同行列

```
機械学習タブ → 評価 → 混同行列

表示例（3クラス）:
              予測
         A    B    C
実  A  [45   3   2]
際  B  [ 2  48   1]
    C  [ 1   2  47]

解釈:
対角線上: 正しい予測
対角線外: 誤り

詳細分析:
- AをBと誤分類: 3サンプル
- BをCと誤分類: 1サンプル
→ どのクラス間で混同が起きているかわかる
```

### ROC曲線とAUC

```
2クラス分類の評価

ROC曲線:
X軸: 偽陽性率（FPR）
Y軸: 真陽性率（TPR = Recall）

AUC（Area Under Curve）:
0.5: ランダム
0.7-0.8: 良好
0.8-0.9: 優秀
> 0.9: 非常に優秀

用途:
- 閾値に依存しない評価
- モデル間の比較
```

### クロスバリデーション結果

```
機械学習タブ → 評価 → CV結果

表示:
Fold 1: 85.3%
Fold 2: 87.8%
Fold 3: 86.1%
Fold 4: 84.9%
Fold 5: 86.5%

平均: 86.1%
標準偏差: 1.1%

解釈:
- 標準偏差が小さい: 安定したモデル
- 標準偏差が大きい: 不安定（過学習の可能性）

目標:
標準偏差 < 3%
```

---

## ハイパーパラメータ調整

### 学習曲線

```
機械学習タブ → 診断 → 学習曲線

表示:
X軸: トレーニングサンプル数
Y軸: スコア

2本の曲線:
- トレーニングスコア
- 検証スコア

診断:

パターン1: 過学習
訓練スコア: 高い（95%）
検証スコア: 低い（70%）
→ 正則化を増やす、データを増やす

パターン2: 過小適合
訓練スコア: 低い（65%）
検証スコア: 低い（60%）
→ より複雑なモデル、特徴量を追加

パターン3: 良好
訓練スコア: 高い（90%）
検証スコア: やや低い（85%）
→ バランスが取れている
```

### 検証曲線

```
機械学習タブ → 診断 → 検証曲線

パラメータを選択（例: max_depth）

表示:
X軸: パラメータ値
Y軸: スコア

2本の曲線:
- トレーニングスコア
- 検証スコア

最適値の特定:
検証スコアが最大のパラメータ値を選択
```

---

## 特徴量選択

### 特徴量の重要度

```
機械学習タブ → 特徴量の重要度

Random Forest / XGBoostで利用可能

表示:
棒グラフ: 各波数の重要度

解釈:
- 高い値: 分類に重要な波数
- 低い値: あまり重要でない

活用:
1. 重要な波数を特定
2. 化学的解釈（どの結合が重要か）
3. 特徴量選択（重要度の低い波数を除去）
```

### RFE（Recursive Feature Elimination）

```
設定 → 特徴量選択 → RFE

手順:
1. すべての特徴量でモデルを訓練
2. 最も重要度の低い特徴量を削除
3. 繰り返す
4. 指定した数の特徴量まで削減

パラメータ:
n_features_to_select: 50-100

利点:
- モデル性能の維持
- 計算時間の短縮
- 過学習の防止
```

### 分散閾値法

```
設定 → 特徴量選択 → 分散閾値

原理:
分散が低い特徴量（ほぼ定数）を除去

閾値: 0.01

理由:
分散が低い = すべてのサンプルで類似
→ 識別に役立たない
```

---

## モデルの解釈

### SHAP値

```
機械学習タブ → 解釈 → SHAP

SHapley Additive exPlanations

表示:
各特徴量の各サンプルへの寄与

サマリープロット:
- 各特徴量の重要度
- 正/負の寄与

フォースプロット:
特定のサンプルの予測理由を視覚化

用途:
- モデルのブラックボックスを開く
- 予測の説明
- 信頼性の向上
```

### 混同しやすいサンプルの分析

```
機械学習タブ → 診断 → 誤分類分析

表示:
誤って分類されたサンプルのリスト

分析:
1. これらのサンプルの共通点
2. スペクトルの特徴
3. データ品質の問題

対策:
- より多くの類似サンプルを収集
- 前処理の改善
- より複雑なモデルの使用
```

---

## 予測と実運用

### 新しいデータへの予測

```
機械学習タブ → 予測

ステップ:
1. 新しいデータをロード
2. 同じ前処理を適用（自動）
3. 予測を実行

結果:
- 予測クラス
- 確率（各クラスの確率）
- 信頼度

例:
サンプル1:
  予測: クラスA
  確率: A=0.85, B=0.10, C=0.05
  信頼度: 高

サンプル2:
  予測: クラスB
  確率: A=0.35, B=0.40, C=0.25
  信頼度: 低（要確認）
```

### モデルの保存と読み込み

```
モデル → 保存:
ファイル名: model_20260124.pkl

含まれる内容:
- トレーニング済みモデル
- 前処理パイプライン
- スケーラー
- メタデータ

読み込み:
機械学習タブ → モデルを読み込む →
ファイルを選択 → 予測に使用
```

### バッチ予測

```
機械学習タブ → バッチ予測

手順:
1. 複数のファイルを選択
2. 自動的に前処理と予測
3. 結果をエクスポート

出力（CSV）:
Filename,PredictedClass,Prob_A,Prob_B,Prob_C,Confidence
sample001.csv,A,0.85,0.10,0.05,High
sample002.csv,B,0.15,0.75,0.10,High
sample003.csv,C,0.20,0.30,0.50,Medium
```

### モデルの更新

```
状況:
新しいデータが利用可能になった

オプション1: 再トレーニング
すべてのデータで新しいモデルを訓練

オプション2: 増分学習
既存モデルに新しいデータを追加
（一部のアルゴリズムのみ対応）

推奨:
定期的に再トレーニング（月1回など）
```

---

## 🔗 関連ドキュメント

- **[前処理ガイド](preprocessing_ja.md)** - データの前処理
- **[分析ガイド](analysis_ja.md)** - 探索的分析
- **[FAQ](../faq_ja.md)** - よくある質問
- **[トラブルシューティング](../troubleshooting_ja.md)** - 問題解決

---

**最終更新**: 2026年1月24日 | **バージョン**: 1.0.0

import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    mean_squared_error, r2_score, confusion_matrix,
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List
import pickle
from notebook_utils.visualize import MLVisualize

# LINEAR REGRESSION MODEL


class LinearRegressionModel:
    """
    A class for training and evaluating a Linear Regression model on Raman spectroscopy data.

    Note: Since the target labels are categorical (e.g., 'MGUS', 'MM'), this model treats
    the problem as regression by encoding labels numerically. For classification tasks,
    consider using LogisticRegression instead.
    """

    def __init__(self, data_split: Dict[str, Any], **kwargs):
        """
        Initialize the Linear Regression model.

        Args:
            data_split (dict): Data dictionary from RamanDataSplitter.prepare_data()
            **kwargs: Additional keyword arguments for LinearRegression
        """
        self.data_split = data_split

        # check for missing keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        self.model = LinearRegression(**kwargs)
        self.label_encoder = LabelEncoder()

        # Encode labels numerically for regression
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.

        Returns:
            Dict[str, Any]: Evaluation metrics including regression and classification metrics
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Regression metrics
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        print("=== Detailed Evaluation Metrics ===")
        print("\nRegression Metrics:")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  R² Score: {r2:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def fit(self) -> None:
        """
        Fit the Linear Regression model to the training data.
        """
        print("Fitting Linear Regression model...")
        self.model.fit(self.data_split['X_train'], self.y_train_encoded)
        print("Model fitted successfully.")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        return self.model.predict(X)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        # Round to nearest integer for label decoding
        predictions_rounded = np.round(predictions_encoded).astype(int)
        # Clip to valid label range
        predictions_rounded = np.clip(
            predictions_rounded, 0, len(self.label_encoder.classes_) - 1)
        return self.label_encoder.inverse_transform(predictions_rounded)

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix using sklearn's official implementation.

        Args:
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix (Counts and Percentages) LINEAR REGRESSION')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None, show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print("\nRegression Metrics:")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.

        Args:
            y_true (np.ndarray): True labels
            y_pred (List[str]): Predicted labels
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix for New Data (Counts and Percentages) LINEAR REGRESSION')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and encoder to a pickle file.

        Args:
            filepath (str): Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'LinearRegressionModel':
        """
        Load a trained model from a pickle file.

        Args:
            filepath (str): Path to the saved model file

        Returns:
            LinearRegressionModel: Loaded model instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = LinearRegressionModel(model_data['data_split'])
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        return instance

# LOGISTIC REGRESSION MODEL


class LogisticRegressionModel:
    """
    A class for training and evaluating a Logistic Regression model on Raman spectroscopy data.

    This class provides proper classification using logistic regression instead of treating
    categorical labels as regression targets. Logistic regression is more appropriate for
    classification tasks as it:
    - Uses probability-based predictions (0-1 range)
    - Handles categorical outcomes naturally
    - Provides class probabilities for uncertainty quantification
    - Offers better interpretability for classification decisions

    Based on research applications:
    - Zeng et al. (2022): Logistic regression for SERS-based miRNA classification
    - Chia et al. (2020): Interpretable classification of bacterial Raman spectra
    - Lancia et al. (2023): Logistic regression models for genomic DNA classification
    """

    def __init__(self, data_split: Dict[str, Any],
                 # Logistic Regression specific parameters
                 penalty: str = 'l2',
                 C: float = 1.0,
                 solver: str = 'lbfgs',
                 max_iter: int = 1000,
                 multi_class: str = 'auto',
                 class_weight: Optional[str] = None,
                 random_state: Optional[int] = 42,
                 # Scaling parameters
                 scale_features: bool = True,
                 **kwargs):
        """
        Initialize the Logistic Regression model.

        Args:
            data_split: Data dictionary from RamanDataSplitter.prepare_data()
            penalty: Regularization penalty ('l1', 'l2', 'elasticnet', 'none')
            C: Inverse regularization strength (smaller = more regularization)
            solver: Algorithm for optimization ('lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga')
            max_iter: Maximum iterations for solver convergence
            multi_class: Multi-class strategy ('auto', 'ovr', 'multinomial')
            class_weight: Class weights for imbalanced data ('balanced', dict, or None)
            random_state: Random seed for reproducible results
            scale_features: Whether to standardize features (recommended for logistic regression)
            **kwargs: Additional keyword arguments for LogisticRegression
        """
        self.data_split = data_split
        self.scale_features = scale_features

        # Check for missing keys (same as LinearRegressionModel)
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        # Initialize logistic regression model with medical-appropriate parameters
        self.model = LogisticRegression(
            penalty=penalty,
            C=C,
            solver=solver,
            max_iter=max_iter,
            multi_class=multi_class,
            class_weight=class_weight,
            random_state=random_state,
            **kwargs
        )

        # Label encoder for consistent interface
        self.label_encoder = LabelEncoder()
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        # Feature scaler for logistic regression (important for convergence)
        self.scaler = None
        if self.scale_features:
            self.scaler = StandardScaler()

        # Performance tracking
        self.training_history = {}

        print("=== Logistic Regression Model for Raman Spectroscopy ===")
        print(f"Classes: {list(self.label_encoder.classes_)}")
        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")
        print(f"Training samples: {len(self.data_split['X_train'])}")
        print(f"Test samples: {len(self.data_split['X_test'])}")
        print(f"Features: {self.data_split['X_train'].shape[1]}")
        print(f"Solver: {solver}, Penalty: {penalty}, C: {C}")
        print(
            f"Class weight: {class_weight}, Feature scaling: {scale_features}")
        if class_weight == 'balanced':
            print("Note: Using balanced class weights for imbalanced data handling")

    def fit(self) -> None:
        """
        Fit the Logistic Regression model to the training data.
        """
        print("\nFitting Logistic Regression model...")

        # Prepare training data
        X_train = self.data_split['X_train'].copy()

        # Apply feature scaling if enabled
        if self.scale_features:
            print("Applying feature standardization...")
            X_train = self.scaler.fit_transform(X_train)

        # Fit the logistic regression model
        try:
            self.model.fit(X_train, self.y_train_encoded)
            print("Model fitted successfully.")

            # Store training information
            self.training_history = {
                'n_iter': getattr(self.model, 'n_iter_', 'N/A'),
                'classes': self.model.classes_,
                'n_features_in': self.model.n_features_in_,
                'feature_names_in': getattr(self.model, 'feature_names_in_', None)
            }

            # Print convergence information
            if hasattr(self.model, 'n_iter_'):
                if isinstance(self.model.n_iter_, np.ndarray):
                    print(
                        f"Convergence achieved in {self.model.n_iter_[0]} iterations")
                else:
                    print(
                        f"Convergence achieved in {self.model.n_iter_} iterations")

        except Exception as e:
            print(f"Error during model fitting: {e}")
            print("Try adjusting solver, max_iter, or C parameter")
            raise e

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X: Feature matrix

        Returns:
            Predicted encoded class labels
        """
        # Apply same scaling as training data
        if self.scale_features and self.scaler is not None:
            X_scaled = self.scaler.transform(X)
        else:
            X_scaled = X

        # Get class predictions (encoded)
        return self.model.predict(X_scaled)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X: Feature matrix

        Returns:
            Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class probabilities for samples.

        Args:
            X: Feature matrix

        Returns:
            Class probabilities for each sample
        """
        # Apply same scaling as training data
        if self.scale_features and self.scaler is not None:
            X_scaled = self.scaler.transform(X)
        else:
            X_scaled = X

        return self.model.predict_proba(X_scaled)

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        """
        Compute decision function (confidence scores) for samples.

        Args:
            X: Feature matrix

        Returns:
            Decision function values
        """
        # Apply same scaling as training data
        if self.scale_features and self.scaler is not None:
            X_scaled = self.scaler.transform(X)
        else:
            X_scaled = X

        return self.model.decision_function(X_scaled)

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.
        Same interface as LinearRegressionModel but with proper logistic regression metrics.

        Returns:
            Dictionary containing evaluation metrics
        """
        # Prepare test data
        X_test = self.data_split['X_test']
        if self.scale_features and self.scaler is not None:
            X_test = self.scaler.transform(X_test)

        # Get predictions and probabilities
        y_pred_encoded = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)
        y_pred_labels = self.label_encoder.inverse_transform(y_pred_encoded)
        y_true_labels = self.data_split['y_test']

        # === Classification Metrics ===
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        # === Logistic Regression Specific Metrics ===
        # Log loss (cross-entropy loss)
        log_loss_score = log_loss(self.y_test_encoded, y_pred_proba)

        # AUC-ROC (for binary classification or multi-class OvR)
        try:
            if len(self.label_encoder.classes_) == 2:
                # Binary classification
                auc_roc = roc_auc_score(
                    self.y_test_encoded, y_pred_proba[:, 1])
            else:
                # Multi-class (One-vs-Rest)
                auc_roc = roc_auc_score(
                    self.y_test_encoded, y_pred_proba, multi_class='ovr', average='weighted')
        except ValueError:
            auc_roc = None

        # === Regression-like Metrics (for interface compatibility) ===
        # Treat predicted probabilities as "continuous" values for regression metrics
        if len(self.label_encoder.classes_) == 2:
            # Binary: use probability of positive class
            continuous_predictions = y_pred_proba[:, 1]
            continuous_targets = self.y_test_encoded.astype(float)
        else:
            # Multi-class: use max probability
            continuous_predictions = np.max(y_pred_proba, axis=1)
            continuous_targets = self.y_test_encoded.astype(
                float) / (len(self.label_encoder.classes_) - 1)

        mse = mean_squared_error(continuous_targets, continuous_predictions)
        r2 = r2_score(continuous_targets, continuous_predictions)

        # Compile metrics
        metrics = {
            'regression': {  # For interface compatibility
                'mean_squared_error': mse,
                'r2_score': r2,
                'note': 'Regression metrics computed from probabilities for compatibility'
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            },
            'logistic_regression': {
                'log_loss': log_loss_score,
                'auc_roc': auc_roc,
                'n_iterations': self.training_history.get('n_iter', 'N/A'),
                'solver_converged': True  # If we reach here, it converged
            }
        }

        # Print detailed results
        print("=== Detailed Evaluation Metrics (Logistic Regression) ===")
        print("\nLogistic Regression Metrics:")
        print(f"  Log Loss (Cross-entropy): {log_loss_score:.4f}")
        if auc_roc is not None:
            print(f"  AUC-ROC: {auc_roc:.4f}")
        print(
            f"  Solver Iterations: {self.training_history.get('n_iter', 'N/A')}")

        print(f"\nAccuracy: {accuracy:.4f}")
        print(f"F1-weighted: {f1_weighted:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        # Print class probability statistics
        print("\nPrediction Confidence Statistics:")
        max_probs = np.max(y_pred_proba, axis=1)
        print(f"  Mean confidence: {np.mean(max_probs):.3f}")
        print(f"  Min confidence: {np.min(max_probs):.3f}")
        print(f"  Max confidence: {np.max(max_probs):.3f}")
        print(f"  Std confidence: {np.std(max_probs):.3f}")

        return metrics

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix.
        Same interface as LinearRegressionModel.

        Args:
            show_plot: Whether to display the plot

        Returns:
            Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text (same format as LinearRegressionModel)
        print("\nConfusion Matrix (Text) for Test Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))

        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix (Counts and Percentages) LOGISTIC REGRESSION')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')

            plt.tight_layout()
            plt.show()

        return cm

    def plot_probability_distributions(self) -> None:
        """
        Plot prediction probability distributions for each class.
        Unique to LogisticRegressionModel - shows model confidence.
        """
        # Get predictions for test data
        X_test = self.data_split['X_test']
        if self.scale_features and self.scaler is not None:
            X_test = self.scaler.transform(X_test)

        y_pred_proba = self.model.predict_proba(X_test)
        y_true = self.data_split['y_test']

        n_classes = len(self.label_encoder.classes_)
        fig, axes = plt.subplots(1, n_classes, figsize=(5*n_classes, 4))

        if n_classes == 1:
            axes = [axes]

        for i, class_name in enumerate(self.label_encoder.classes_):
            ax = axes[i] if n_classes > 1 else axes[0]

            # Get probabilities for this class
            class_probs = y_pred_proba[:, i]

            # Separate by true class
            true_class_mask = y_true == class_name
            false_class_mask = ~true_class_mask

            # Plot histograms
            ax.hist(class_probs[true_class_mask], alpha=0.7, label=f'True {class_name}',
                    bins=20, color='green', density=True)
            ax.hist(class_probs[false_class_mask], alpha=0.7, label=f'True Other',
                    bins=20, color='red', density=True)

            ax.set_xlabel(f'Predicted Probability for {class_name}')
            ax.set_ylabel('Density')
            ax.set_title(f'Probability Distribution: {class_name}')
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.suptitle(
            'Prediction Probability Distributions by True Class', y=1.02, fontsize=14)
        plt.show()

    def plot_roc_curves(self) -> None:
        """
        Plot ROC curves for binary or multi-class classification.
        Unique to LogisticRegressionModel.
        """
        # Get predictions for test data
        X_test = self.data_split['X_test']
        if self.scale_features and self.scaler is not None:
            X_test = self.scaler.transform(X_test)

        y_pred_proba = self.model.predict_proba(X_test)
        y_true = self.y_test_encoded

        n_classes = len(self.label_encoder.classes_)

        plt.figure(figsize=(8, 6))

        if n_classes == 2:
            # Binary classification
            fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
            auc = roc_auc_score(y_true, y_pred_proba[:, 1])

            plt.plot(fpr, tpr, linewidth=2,
                     label=f'ROC Curve (AUC = {auc:.3f})')

        else:
            # Multi-class (One-vs-Rest)
            for i, class_name in enumerate(self.label_encoder.classes_):
                y_true_binary = (y_true == i).astype(int)
                fpr, tpr, _ = roc_curve(y_true_binary, y_pred_proba[:, i])
                auc = roc_auc_score(y_true_binary, y_pred_proba[:, i])

                plt.plot(fpr, tpr, linewidth=2,
                         label=f'{class_name} (AUC = {auc:.3f})')

        # Plot diagonal line
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')

        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves - Logistic Regression')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xlim([0, 1])
        plt.ylim([0, 1])
        plt.show()

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None,
                         show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.
        Same interface as LinearRegressionModel.

        Args:
            X_new: New feature matrix for prediction
            y_new: True labels for new data (if available)  
            show_confusion_matrix: Whether to display the confusion matrix

        Returns:
            Dictionary containing predictions and evaluation metrics
        """
        # Get predictions and probabilities
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)
        predictions_proba = self.predict_proba(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'predictions_probabilities': predictions_proba,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Logistic Regression Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        # Show prediction confidence
        max_probs = np.max(predictions_proba, axis=1)
        print(f"Prediction confidence - Mean: {np.mean(max_probs):.3f}, "
              f"Min: {np.min(max_probs):.3f}, Max: {np.max(max_probs):.3f}")

        if y_new is not None:
            # Evaluate predictions

            # === Classification Metrics ===
            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            # === Logistic Regression Metrics ===
            y_new_encoded = self.label_encoder.transform(y_new)
            log_loss_score = log_loss(y_new_encoded, predictions_proba)

            # AUC-ROC
            try:
                if len(self.label_encoder.classes_) == 2:
                    auc_roc = roc_auc_score(
                        y_new_encoded, predictions_proba[:, 1])
                else:
                    auc_roc = roc_auc_score(
                        y_new_encoded, predictions_proba, multi_class='ovr', average='weighted')
            except ValueError:
                auc_roc = None

            # === Regression-like Metrics (for compatibility) ===
            if len(self.label_encoder.classes_) == 2:
                continuous_predictions = predictions_proba[:, 1]
                continuous_targets = y_new_encoded.astype(float)
            else:
                continuous_predictions = np.max(predictions_proba, axis=1)
                continuous_targets = y_new_encoded.astype(
                    float) / (len(self.label_encoder.classes_) - 1)

            mse = mean_squared_error(
                continuous_targets, continuous_predictions)
            r2 = r2_score(continuous_targets, continuous_predictions)

            result['evaluation'] = {
                'regression': {  # For compatibility
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                },
                'logistic_regression': {
                    'log_loss': log_loss_score,
                    'auc_roc': auc_roc
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print(f"\nAccuracy: {accuracy:.4f}")
            print(f"Log Loss: {log_loss_score:.4f}")
            if auc_roc is not None:
                print(f"AUC-ROC: {auc_roc:.4f}")
            print(f"F1-weighted: {f1_weighted:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str]) -> np.ndarray:
        """
        Plot confusion matrix for new data.
        Same interface as LinearRegressionModel.
        """
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Text output (same format)
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))

        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        # Plot
        report = classification_report(
            y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        total = cm.sum()
        annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                         for j in range(cm.shape[1])]
                         for i in range(cm.shape[0])])

        sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                    xticklabels=self.label_encoder.classes_,
                    yticklabels=self.label_encoder.classes_, ax=ax1)
        ax1.set_title(
            'Confusion Matrix for New Data (Counts and Percentages) LOGISTIC REGRESSION')
        ax1.set_ylabel('True Label')
        ax1.set_xlabel('Predicted Label')

        ax2.text(0.1, 0.5, report, fontsize=10,
                 verticalalignment='center', fontfamily='monospace')
        ax2.set_title('Classification Report for New Data')
        ax2.axis('off')

        plt.tight_layout()
        plt.show()

        return cm

    def get_feature_importance(self, top_n: int = 20) -> Dict[str, Any]:
        """
        Get feature importance based on logistic regression coefficients.

        Args:
            top_n: Number of top features to return

        Returns:
            Dictionary with feature importance information
        """
        if not hasattr(self.model, 'coef_'):
            raise ValueError(
                "Model must be fitted before getting feature importance")

        coefficients = self.model.coef_
        wavelengths = self.data_split['unified_wavelengths']

        if len(self.label_encoder.classes_) == 2:
            # Binary classification - single coefficient vector
            coef_abs = np.abs(coefficients[0])
            feature_importance = [
                {
                    'wavelength': wavelengths[i],
                    'coefficient': coefficients[0][i],
                    'abs_coefficient': coef_abs[i],
                    'rank': rank
                }
                for rank, i in enumerate(np.argsort(coef_abs)[::-1], 1)
            ]
        else:
            # Multi-class classification - average absolute coefficients across classes
            coef_abs = np.mean(np.abs(coefficients), axis=0)
            feature_importance = [
                {
                    'wavelength': wavelengths[i],
                    'coefficients': {class_name: coefficients[j][i]
                                     for j, class_name in enumerate(self.label_encoder.classes_)},
                    'avg_abs_coefficient': coef_abs[i],
                    'rank': rank
                }
                for rank, i in enumerate(np.argsort(coef_abs)[::-1], 1)
            ]

        return {
            'feature_importance': feature_importance[:top_n],
            'n_features': len(feature_importance),
            'is_binary': len(self.label_encoder.classes_) == 2
        }

    def plot_feature_importance(self, top_n: int = 20) -> None:
        """
        Plot feature importance based on logistic regression coefficients.
        """
        importance_data = self.get_feature_importance(top_n)
        features = importance_data['feature_importance']

        if importance_data['is_binary']:
            # Binary classification plot
            wavelengths = [f['wavelength'] for f in features]
            coefficients = [f['coefficient'] for f in features]

            plt.figure(figsize=(12, 6))
            colors = ['red' if c < 0 else 'blue' for c in coefficients]

            plt.barh(range(len(wavelengths)),
                     coefficients, color=colors, alpha=0.7)
            plt.yticks(range(len(wavelengths)), [
                       f"{w:.0f} cm⁻¹" for w in wavelengths])
            plt.xlabel('Logistic Regression Coefficient')
            plt.title(
                f'Top {top_n} Most Important Features (Binary Classification)')
            plt.grid(True, alpha=0.3, axis='x')

            # Add legend
            import matplotlib.patches as mpatches
            red_patch = mpatches.Patch(
                color='red', alpha=0.7, label=f'Favors {self.label_encoder.classes_[0]}')
            blue_patch = mpatches.Patch(
                color='blue', alpha=0.7, label=f'Favors {self.label_encoder.classes_[1]}')
            plt.legend(handles=[red_patch, blue_patch])

        else:
            # Multi-class classification plot
            wavelengths = [f['wavelength'] for f in features]
            avg_coeffs = [f['avg_abs_coefficient'] for f in features]

            plt.figure(figsize=(12, 6))
            plt.barh(range(len(wavelengths)), avg_coeffs,
                     color='green', alpha=0.7)
            plt.yticks(range(len(wavelengths)), [
                       f"{w:.0f} cm⁻¹" for w in wavelengths])
            plt.xlabel('Average Absolute Coefficient')
            plt.title(
                f'Top {top_n} Most Important Features (Multi-class Classification)')
            plt.grid(True, alpha=0.3, axis='x')

        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and components to a pickle file.
        Same interface as LinearRegressionModel.

        Args:
            filepath: Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'scaler': self.scaler,
            'data_split': self.data_split,
            'training_history': self.training_history,
            'scale_features': self.scale_features
        }

        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Logistic Regression model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'LogisticRegressionModel':
        """
        Load a trained model from a pickle file.
        Same interface as LinearRegressionModel.

        Args:
            filepath: Path to the saved model file

        Returns:
            Loaded LogisticRegressionModel instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        # Reconstruct instance
        instance = LogisticRegressionModel(
            model_data['data_split'],
            scale_features=model_data.get('scale_features', True)
        )
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        instance.scaler = model_data.get('scaler', None)
        instance.training_history = model_data.get('training_history', {})

        print(f"Logistic Regression model loaded from {filepath}")
        return instance

# BIOMARKER-ENHANCED LINEAR REGRESSION MODEL FOR MGUS/MM CLASSIFICATION


class BiomarkerEnhancedLinearRegressionModel:
    """
    Biomarker-Enhanced Linear Regression Model for MGUS/MM Raman Spectroscopy Classification.

    Based on clinical research findings:
    - Yonezawa et al. (2024): Primary DNB markers at 1149 cm⁻¹ and 1527-1530 cm⁻¹
    - Russo et al. (2020): Disease progression biomarkers
    - Clinical validation on 834 Normal, 711 MGUS, and 970 MM spectra

    This model extracts clinically-validated biomarker features from Raman spectra
    for improved external generalization and clinical interpretability.
    """

    def __init__(self, data_split: Dict[str, Any],
                 use_ridge: bool = True,
                 alpha: float = 1.0,
                 biomarker_only: bool = False,
                 verbose: bool = True,
                 **kwargs):
        """
        Initialize the Biomarker-Enhanced Linear Regression model.

        Args:
            data_split: Data dictionary from RamanDataSplitter.prepare_data()
            use_ridge: Use Ridge regression for better stability
            alpha: Ridge regularization parameter
            biomarker_only: Use only biomarker features (no full spectrum)
            verbose: Print detailed information
            **kwargs: Additional keyword arguments for LinearRegression/Ridge
        """
        self.data_split = data_split
        self.use_ridge = use_ridge
        self.alpha = alpha
        self.biomarker_only = biomarker_only
        self.verbose = verbose

        # Check for missing keys (same as original)
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        # Initialize model (Ridge for better stability with biomarkers)
        if use_ridge:
            self.model = Ridge(alpha=alpha, **kwargs)
            model_type = f"Ridge(α={alpha})"
        else:
            self.model = LinearRegression(**kwargs)
            model_type = "LinearRegression"

        # Label encoding (same as original)
        self.label_encoder = LabelEncoder()
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        # Biomarker extraction components
        self.biomarker_features_train = None
        self.biomarker_features_test = None
        self.biomarker_feature_names = None
        self.feature_importance_analysis = None

        if self.verbose:
            print("=== Biomarker-Enhanced Linear Regression Model ===")
            print(f"Model type: {model_type}")
            print(f"Biomarker-only mode: {biomarker_only}")
            print(f"Training samples: {len(self.data_split['X_train'])}")
            print(f"Test samples: {len(self.data_split['X_test'])}")
            print(f"Original features: {self.data_split['X_train'].shape[1]}")
            print(
                f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")

        # Extract biomarker features during initialization
        self._extract_biomarker_features()

    def _extract_biomarker_features(self) -> None:
        """
        Extract clinically-validated MGUS/MM biomarker features from Raman spectra.

        Based on literature review of MGUS/MM Raman spectroscopy biomarkers:
        - Primary DNB markers: 1149, 1527-1530 cm⁻¹ (Yonezawa et al., 2024)
        - Nucleic acid markers: 726, 781, 786, 1078, 1190, 1415 cm⁻¹
        - Protein markers: 1004, 1221, 1655 cm⁻¹
        - Lipid/metabolic markers: 1285, 1440 cm⁻¹
        """
        wavelengths = self.data_split['unified_wavelengths']

        # Define clinically-validated biomarker wavelengths
        biomarker_bands = {
            # Primary DNB (Dynamical Network Biomarker) markers - Yonezawa et al. 2024
            'DNB_primary_1149': 1149,      # Primary MGUS/MM discriminator
            # Primary MGUS/MM discriminator (1527-1530 range)
            'DNB_primary_1528': 1528,

            # Nucleic acid markers (DNA/RNA changes in cancer)
            'nucleic_acid_726': 726,       # Adenine, thymine
            'nucleic_acid_781': 781,       # DNA/RNA backbone
            'nucleic_acid_786': 786,       # DNA/RNA backbone
            'nucleic_backbone_1078': 1078,  # PO₄⁻ symmetric stretch
            'nucleic_backbone_1190': 1190,  # DNA/RNA backbone vibrations
            'nucleic_backbone_1415': 1415,  # DNA/RNA backbone

            # Protein structure markers
            # Phe ring breathing (protein stress)
            'phenylalanine_1004': 1004,
            'protein_amide_III_1221': 1221,  # Protein structure changes
            'protein_amide_I_1655': 1655,  # Protein α-helix/β-sheet

            # Lipid and metabolic markers
            'phospholipid_1285': 1285,     # Phospholipid/cholesterol
            'lipid_CH2_1440': 1440,       # CH₂ deformation (membrane changes)

            # Background/reference bands
            'background_low_600': 600,     # Background reference
            'background_high_1800': 1800   # Background reference
        }

        if self.verbose:
            print(
                f"\n=== Extracting {len(biomarker_bands)} Biomarker Features ===")

        # Extract band intensities for training data
        biomarker_intensities_train = {}
        biomarker_intensities_test = {}

        for band_name, target_wavenumber in biomarker_bands.items():
            # Find closest wavelength index
            idx = np.argmin(np.abs(wavelengths - target_wavenumber))
            actual_wavenumber = wavelengths[idx]

            # Extract intensities
            biomarker_intensities_train[band_name] = self.data_split['X_train'][:, idx]
            biomarker_intensities_test[band_name] = self.data_split['X_test'][:, idx]

            if self.verbose and abs(actual_wavenumber - target_wavenumber) > 5:
                print(
                    f"Warning: {band_name} target={target_wavenumber} cm⁻¹, actual={actual_wavenumber:.1f} cm⁻¹")

        # Create ratio features (batch-invariant and clinically meaningful)
        ratio_features_train = {
            # Primary DNB ratio (most important clinical marker)
            'DNB_ratio_1149_1528': (biomarker_intensities_train['DNB_primary_1149'] /
                                    (biomarker_intensities_train['DNB_primary_1528'] + 1e-8)),

            # Nucleic acid to protein ratios (cell proliferation markers)
            'nucleic_protein_ratio': (np.mean([biomarker_intensities_train['nucleic_acid_726'],
                                              biomarker_intensities_train['nucleic_acid_781']], axis=0) /
                                      (biomarker_intensities_train['protein_amide_III_1221'] + 1e-8)),

            # Lipid to protein ratio (metabolic changes)
            'lipid_protein_ratio': (biomarker_intensities_train['lipid_CH2_1440'] /
                                    (biomarker_intensities_train['protein_amide_I_1655'] + 1e-8)),

            # Metabolic stress ratio
            'metabolic_stress_ratio': (biomarker_intensities_train['phenylalanine_1004'] /
                                       (biomarker_intensities_train['phospholipid_1285'] + 1e-8)),

            # Signal-to-background ratios (data quality markers)
            'signal_background_low': (biomarker_intensities_train['DNB_primary_1149'] /
                                      (biomarker_intensities_train['background_low_600'] + 1e-8)),
            'signal_background_high': (biomarker_intensities_train['DNB_primary_1528'] /
                                       (biomarker_intensities_train['background_high_1800'] + 1e-8)),

            # Advanced clinical ratios
            'MGUS_progression_marker': ((biomarker_intensities_train['nucleic_backbone_1078'] +
                                         biomarker_intensities_train['nucleic_backbone_1190']) /
                                        (biomarker_intensities_train['protein_amide_I_1655'] + 1e-8)),

            'MM_severity_marker': (biomarker_intensities_train['nucleic_acid_786'] /
                                   (biomarker_intensities_train['DNB_primary_1149'] + 1e-8))
        }

        # Create corresponding test ratios
        ratio_features_test = {
            'DNB_ratio_1149_1528': (biomarker_intensities_test['DNB_primary_1149'] /
                                    (biomarker_intensities_test['DNB_primary_1528'] + 1e-8)),
            'nucleic_protein_ratio': (np.mean([biomarker_intensities_test['nucleic_acid_726'],
                                              biomarker_intensities_test['nucleic_acid_781']], axis=0) /
                                      (biomarker_intensities_test['protein_amide_III_1221'] + 1e-8)),
            'lipid_protein_ratio': (biomarker_intensities_test['lipid_CH2_1440'] /
                                    (biomarker_intensities_test['protein_amide_I_1655'] + 1e-8)),
            'metabolic_stress_ratio': (biomarker_intensities_test['phenylalanine_1004'] /
                                       (biomarker_intensities_test['phospholipid_1285'] + 1e-8)),
            'signal_background_low': (biomarker_intensities_test['DNB_primary_1149'] /
                                      (biomarker_intensities_test['background_low_600'] + 1e-8)),
            'signal_background_high': (biomarker_intensities_test['DNB_primary_1528'] /
                                       (biomarker_intensities_test['background_high_1800'] + 1e-8)),
            'MGUS_progression_marker': ((biomarker_intensities_test['nucleic_backbone_1078'] +
                                         biomarker_intensities_test['nucleic_backbone_1190']) /
                                        (biomarker_intensities_test['protein_amide_I_1655'] + 1e-8)),
            'MM_severity_marker': (biomarker_intensities_test['nucleic_acid_786'] /
                                   (biomarker_intensities_test['DNB_primary_1149'] + 1e-8))
        }

        # Combine absolute intensities and ratios
        all_features_train = {
            **biomarker_intensities_train, **ratio_features_train}
        all_features_test = {
            **biomarker_intensities_test, **ratio_features_test}

        # Convert to arrays
        self.biomarker_features_train = np.column_stack(
            list(all_features_train.values()))
        self.biomarker_features_test = np.column_stack(
            list(all_features_test.values()))
        self.biomarker_feature_names = list(all_features_train.keys())

        # Optional: Combine with original features
        if not self.biomarker_only:
            self.biomarker_features_train = np.column_stack([
                self.data_split['X_train'],
                self.biomarker_features_train
            ])
            self.biomarker_features_test = np.column_stack([
                self.data_split['X_test'],
                self.biomarker_features_test
            ])

            # Add original feature names
            original_feature_names = [f"wn_{wn:.1f}" for wn in wavelengths]
            self.biomarker_feature_names = original_feature_names + self.biomarker_feature_names

        if self.verbose:
            print(f"Biomarker feature extraction completed:")
            print(
                f"  Training features: {self.biomarker_features_train.shape}")
            print(f"  Test features: {self.biomarker_features_test.shape}")
            print(
                f"  Total biomarker features: {len(biomarker_bands) + len(ratio_features_train)}")

            # Show key biomarker statistics
            print(f"\n=== Key Biomarker Statistics ===")
            for class_name in self.label_encoder.classes_:
                class_mask = self.data_split['y_train'] == class_name
                dnb_1149_mean = np.mean(
                    biomarker_intensities_train['DNB_primary_1149'][class_mask])
                dnb_1528_mean = np.mean(
                    biomarker_intensities_train['DNB_primary_1528'][class_mask])
                dnb_ratio_mean = np.mean(
                    ratio_features_train['DNB_ratio_1149_1528'][class_mask])

                print(
                    f"{class_name}: DNB_1149={dnb_1149_mean:.3f}, DNB_1528={dnb_1528_mean:.3f}, Ratio={dnb_ratio_mean:.3f}")

    def _extract_biomarker_features_from_external(self, X_external: np.ndarray) -> np.ndarray:
        """
        Extract biomarker features from external data using same methodology.

        Args:
            X_external: External spectral data

        Returns:
            Extracted biomarker features
        """
        wavelengths = self.data_split['unified_wavelengths']

        # Same biomarker bands as in initialization
        biomarker_bands = {
            'DNB_primary_1149': 1149, 'DNB_primary_1528': 1528,
            'nucleic_acid_726': 726, 'nucleic_acid_781': 781, 'nucleic_acid_786': 786,
            'nucleic_backbone_1078': 1078, 'nucleic_backbone_1190': 1190, 'nucleic_backbone_1415': 1415,
            'phenylalanine_1004': 1004, 'protein_amide_III_1221': 1221, 'protein_amide_I_1655': 1655,
            'phospholipid_1285': 1285, 'lipid_CH2_1440': 1440,
            'background_low_600': 600, 'background_high_1800': 1800
        }

        # Extract intensities
        biomarker_intensities = {}
        for band_name, target_wavenumber in biomarker_bands.items():
            idx = np.argmin(np.abs(wavelengths - target_wavenumber))
            biomarker_intensities[band_name] = X_external[:, idx]

        # Create ratio features
        ratio_features = {
            'DNB_ratio_1149_1528': (biomarker_intensities['DNB_primary_1149'] /
                                    (biomarker_intensities['DNB_primary_1528'] + 1e-8)),
            'nucleic_protein_ratio': (np.mean([biomarker_intensities['nucleic_acid_726'],
                                              biomarker_intensities['nucleic_acid_781']], axis=0) /
                                      (biomarker_intensities['protein_amide_III_1221'] + 1e-8)),
            'lipid_protein_ratio': (biomarker_intensities['lipid_CH2_1440'] /
                                    (biomarker_intensities['protein_amide_I_1655'] + 1e-8)),
            'metabolic_stress_ratio': (biomarker_intensities['phenylalanine_1004'] /
                                       (biomarker_intensities['phospholipid_1285'] + 1e-8)),
            'signal_background_low': (biomarker_intensities['DNB_primary_1149'] /
                                      (biomarker_intensities['background_low_600'] + 1e-8)),
            'signal_background_high': (biomarker_intensities['DNB_primary_1528'] /
                                       (biomarker_intensities['background_high_1800'] + 1e-8)),
            'MGUS_progression_marker': ((biomarker_intensities['nucleic_backbone_1078'] +
                                         biomarker_intensities['nucleic_backbone_1190']) /
                                        (biomarker_intensities['protein_amide_I_1655'] + 1e-8)),
            'MM_severity_marker': (biomarker_intensities['nucleic_acid_786'] /
                                   (biomarker_intensities['DNB_primary_1149'] + 1e-8))
        }

        # Combine features
        all_features = {**biomarker_intensities, **ratio_features}
        biomarker_features_external = np.column_stack(
            list(all_features.values()))

        # Add original features if not biomarker-only mode
        if not self.biomarker_only:
            biomarker_features_external = np.column_stack(
                [X_external, biomarker_features_external])

        return biomarker_features_external

    def fit(self) -> None:
        """
        Fit the Biomarker-Enhanced Linear Regression model to the training data.
        """
        if self.verbose:
            print("\n=== Fitting Biomarker-Enhanced Linear Regression Model ===")

        # Fit on biomarker-enhanced features
        self.model.fit(self.biomarker_features_train, self.y_train_encoded)

        # Analyze feature importance (for Ridge regression)
        if hasattr(self.model, 'coef_'):
            self._analyze_feature_importance()

        if self.verbose:
            print("Model fitted successfully on biomarker-enhanced features.")

    def _analyze_feature_importance(self) -> None:
        """Analyze feature importance for biomarker interpretation."""
        if not hasattr(self.model, 'coef_'):
            return

        coefficients = self.model.coef_
        feature_importance = np.abs(coefficients)

        # Create importance dataframe
        importance_data = []
        for i, (name, importance, coef) in enumerate(zip(self.biomarker_feature_names,
                                                         feature_importance, coefficients)):
            # Determine clinical interpretation
            if 'DNB' in name:
                clinical_meaning = "Primary MGUS/MM biomarker"
            elif 'nucleic' in name:
                clinical_meaning = "DNA/RNA activity (proliferation)"
            elif 'protein' in name:
                clinical_meaning = "Protein structure changes"
            elif 'lipid' in name or 'phospholipid' in name:
                clinical_meaning = "Membrane/metabolic changes"
            elif 'ratio' in name:
                clinical_meaning = "Batch-invariant clinical ratio"
            elif 'background' in name:
                clinical_meaning = "Data quality marker"
            else:
                clinical_meaning = "Spectral feature"

            importance_data.append({
                'feature': name,
                'coefficient': coef,
                'importance': importance,
                'clinical_meaning': clinical_meaning
            })

        # Sort by importance
        importance_data.sort(key=lambda x: x['importance'], reverse=True)
        self.feature_importance_analysis = importance_data

        if self.verbose:
            print(f"\n=== Top 10 Most Important Biomarkers ===")
            for i, data in enumerate(importance_data[:10]):
                print(f"{i+1:2d}. {data['feature'][:25]:25s} | "
                      f"Coef: {data['coefficient']:8.4f} | "
                      f"Imp: {data['importance']:7.4f} | "
                      f"{data['clinical_meaning']}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data using biomarker features.

        Args:
            X: Feature matrix (original spectral data)

        Returns:
            Predicted encoded labels
        """
        # Extract biomarker features from input
        biomarker_features = self._extract_biomarker_features_from_external(X)

        # Make predictions using biomarker-enhanced features
        return self.model.predict(biomarker_features)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X: Feature matrix (original spectral data)

        Returns:
            Predicted original labels
        """
        predictions_encoded = self.predict(X)

        # Round to nearest integer for label decoding
        predictions_rounded = np.round(predictions_encoded).astype(int)

        # Clip to valid label range
        predictions_rounded = np.clip(
            predictions_rounded, 0, len(self.label_encoder.classes_) - 1)

        return self.label_encoder.inverse_transform(predictions_rounded)

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.
        Same interface as original LinearRegressionModel.
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Regression metrics
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        if self.verbose:
            print("=== Detailed Evaluation Metrics (Biomarker-Enhanced) ===")
            print("\nRegression Metrics:")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true_labels, y_pred_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None,
                         show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate. Same interface as original.

        Args:
            X_new: New feature matrix for prediction (original spectral data)
            y_new: True labels for new data (if available)
            show_confusion_matrix: Whether to display the confusion matrix

        Returns:
            Dictionary containing predictions and evaluation metrics
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        if self.verbose:
            print(f"=== Biomarker-Enhanced Predictions on New Data ===")
            print(f"Number of new samples: {len(predictions_labels)}")
            print(
                f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            if self.verbose:
                print("\n=== Evaluation Metrics on New Data ===")
                print("\nRegression Metrics:")
                print(f"  Mean Squared Error: {mse:.4f}")
                print(f"  R² Score: {r2:.4f}")
                print("\nClassification Report:")
                print(classification_report(y_new, predictions_labels,
                      target_names=self.label_encoder.classes_, zero_division=0))

                # Show biomarker analysis for external data
                self._analyze_external_biomarkers(
                    X_new, y_new, predictions_labels)

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _analyze_external_biomarkers(self, X_new: np.ndarray, y_true: np.ndarray, y_pred: List[str]) -> None:
        """Analyze biomarker performance on external data for clinical interpretation."""
        if not self.verbose:
            return

        print("\n=== External Data Biomarker Analysis ===")

        # Extract biomarkers from external data
        external_biomarkers = self._extract_biomarker_features_from_external(
            X_new)

        # Analyze DNB ratio performance
        wavelengths = self.data_split['unified_wavelengths']
        idx_1149 = np.argmin(np.abs(wavelengths - 1149))
        idx_1528 = np.argmin(np.abs(wavelengths - 1528))

        dnb_1149 = X_new[:, idx_1149]
        dnb_1528 = X_new[:, idx_1528]
        dnb_ratio = dnb_1149 / (dnb_1528 + 1e-8)

        # Analyze by true class
        for class_name in self.label_encoder.classes_:
            true_mask = y_true == class_name
            if np.sum(true_mask) == 0:
                continue

            pred_mask = np.array(y_pred) == class_name
            correct_mask = true_mask & pred_mask

            # Statistics for this class
            true_count = np.sum(true_mask)
            correct_count = np.sum(correct_mask)
            accuracy_class = correct_count / true_count if true_count > 0 else 0

            # Biomarker statistics
            dnb_1149_mean = np.mean(dnb_1149[true_mask])
            dnb_1528_mean = np.mean(dnb_1528[true_mask])
            dnb_ratio_mean = np.mean(dnb_ratio[true_mask])

            print(
                f"{class_name}: Accuracy={accuracy_class:.3f} ({correct_count}/{true_count})")
            print(
                f"  DNB_1149: {dnb_1149_mean:.3f}, DNB_1528: {dnb_1528_mean:.3f}, Ratio: {dnb_ratio_mean:.3f}")

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix. Same interface as original.
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text (same as original)
        print("\nConfusion Matrix (Text) for Test Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))

        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix (Biomarker-Enhanced Linear Regression)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str]) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data. Same as original.
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        labels = list(self.label_encoder.classes_) + ["Total"]
        print("Actual |", " | ".join(f"{label:>8}" for label in labels))
        print("-" * (10 + 10 * len(labels)))

        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            total = sum(cm[i, :])
            row.append(f"{total:>8}")
            print(f"{true_label:>6} | {' | '.join(row)}")

        # Generate classification report
        report = classification_report(
            y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

        # Create figure with subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Calculate total for percentages
        total = cm.sum()

        # Create annotation array with counts and percentages
        annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                         for j in range(cm.shape[1])]
                         for i in range(cm.shape[0])])

        # Plot heatmap on ax1
        sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                    xticklabels=self.label_encoder.classes_,
                    yticklabels=self.label_encoder.classes_, ax=ax1)
        ax1.set_title('Confusion Matrix for New Data (Biomarker-Enhanced)')
        ax1.set_ylabel('True Label')
        ax1.set_xlabel('Predicted Label')

        # Plot classification report on ax2
        ax2.text(0.1, 0.5, report, fontsize=10,
                 verticalalignment='center', fontfamily='monospace')
        ax2.set_title('Classification Report for New Data')
        ax2.axis('off')  # Hide axes for text

        plt.tight_layout()
        plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """Save the trained model and all components to a pickle file."""
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split,
            'use_ridge': self.use_ridge,
            'alpha': self.alpha,
            'biomarker_only': self.biomarker_only,
            'biomarker_feature_names': self.biomarker_feature_names,
            'feature_importance_analysis': self.feature_importance_analysis
        }

        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Biomarker-Enhanced model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'BiomarkerEnhancedLinearRegressionModel':
        """Load a trained model from a pickle file."""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = BiomarkerEnhancedLinearRegressionModel(
            model_data['data_split'],
            use_ridge=model_data.get('use_ridge', True),
            alpha=model_data.get('alpha', 1.0),
            biomarker_only=model_data.get('biomarker_only', False),
            verbose=False
        )
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        instance.biomarker_feature_names = model_data.get(
            'biomarker_feature_names', [])
        instance.feature_importance_analysis = model_data.get(
            'feature_importance_analysis', [])
        return instance

    def get_biomarker_report(self) -> Dict[str, Any]:
        """Get comprehensive biomarker analysis report for research documentation."""
        return {
            'model_type': 'Biomarker-Enhanced Linear Regression',
            'regularization': 'Ridge' if self.use_ridge else 'None',
            'alpha': self.alpha if self.use_ridge else 0,
            'biomarker_only_mode': self.biomarker_only,
            'n_biomarker_features': len(self.biomarker_feature_names) if self.biomarker_feature_names else 0,
            'feature_importance': self.feature_importance_analysis,
            'clinical_biomarkers': [
                'DNB_primary_1149: Primary MGUS/MM discriminator',
                'DNB_primary_1528: Primary MGUS/MM discriminator',
                'DNB_ratio_1149_1528: Most important clinical ratio',
                'nucleic_protein_ratio: Cell proliferation marker',
                'MGUS_progression_marker: Disease progression indicator',
                'MM_severity_marker: Disease severity indicator'
            ]
        }


class MGUSAwareBiomarkerModel(BiomarkerEnhancedLinearRegressionModel):
    def __init__(self, data_split, mgus_strategy='conservative', **kwargs):
        super().__init__(data_split, **kwargs)
        self.mgus_strategy = mgus_strategy  # 'conservative', 'two_stage', 'stratified'

    def predict_new_data(self, X_new, y_new=None, **kwargs):
        """Enhanced prediction with MGUS-specific handling."""

        # Get base predictions
        result = super().predict_new_data(X_new, y_new, **kwargs)
        base_predictions = result['predictions_labels']

        # Apply MGUS-specific refinements
        if self.mgus_strategy == 'conservative':
            # Conservative: Classify uncertain MGUS as MM
            refined_predictions = self.apply_mgus_clinical_thresholds(
                self._extract_biomarker_features_from_external(X_new),
                base_predictions
            )

        elif self.mgus_strategy == 'two_stage':
            # Two-stage classification
            refined_predictions = self.predict_two_stage(X_new)

        else:
            refined_predictions = base_predictions

        # Update result
        result['predictions_labels'] = refined_predictions
        result['mgus_strategy'] = self.mgus_strategy

        # Recalculate metrics if ground truth available
        if y_new is not None:
            from sklearn.metrics import accuracy_score, classification_report

            new_accuracy = accuracy_score(y_new, refined_predictions)
            result['evaluation']['classification']['accuracy'] = new_accuracy

            if self.verbose:
                print(f"\n=== MGUS-Aware Classification Results ===")
                print(f"Strategy: {self.mgus_strategy}")
                print(
                    f"Original Accuracy: {accuracy_score(y_new, base_predictions):.3f}")
                print(f"MGUS-Aware Accuracy: {new_accuracy:.3f}")
                print(
                    f"Improvement: {new_accuracy - accuracy_score(y_new, base_predictions):+.3f}")
                print("\nDetailed Report:")
                print(classification_report(y_new, refined_predictions))

        return result


class RamanBiomarkerDatabase:
    """
    External biomarker database for MGUS/MM classification.
    Easy to edit and extend for different research needs.
    """

    def __init__(self):
        self.biomarker_bands = self._define_biomarker_bands()
        self.ratio_definitions = self._define_ratio_features()
        self.clinical_interpretations = self._define_clinical_meanings()

    def _define_biomarker_bands(self) -> Dict[str, Dict]:
        """
        Define individual biomarker bands with metadata.
        Easy to modify wavenumbers and add new biomarkers.
        """
        return {
            # Primary DNB markers (Most Important)
            'DNB_primary_1149': {
                'target_wavenumber': 1149,
                'tolerance': 5,
                'priority': 'CRITICAL',
                'assignment': 'Primary MGUS/MM discriminator',
                'reference': 'Yonezawa et al. (2024)'
            },
            'DNB_primary_1528': {
                'target_wavenumber': 1528,
                'tolerance': 3,
                'priority': 'CRITICAL',
                'assignment': 'Primary MGUS/MM discriminator',
                'reference': 'Yonezawa et al. (2024)'
            },

            # Nucleic acid markers
            'adenine_726': {
                'target_wavenumber': 726,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'Adenine (DNA/RNA)',
                'reference': 'Literature review'
            },
            'nucleic_backbone_781': {
                'target_wavenumber': 781,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'DNA/RNA backbone',
                'reference': 'Literature review'
            },
            'nucleic_backbone_786': {
                'target_wavenumber': 786,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'DNA/RNA backbone',
                'reference': 'Literature review'
            },
            'phosphate_1078': {
                'target_wavenumber': 1078,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'PO₄⁻ symmetric stretch',
                'reference': 'Literature review'
            },
            'nucleic_backbone_1190': {
                'target_wavenumber': 1190,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'DNA/RNA backbone vibrations',
                'reference': 'Literature review'
            },
            'nucleic_backbone_1415': {
                'target_wavenumber': 1415,
                'tolerance': 5,
                'priority': 'MEDIUM',
                'assignment': 'DNA/RNA backbone',
                'reference': 'Literature review'
            },

            # Protein structure markers
            'phenylalanine_1004': {
                'target_wavenumber': 1004,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'Phenylalanine ring breathing',
                'reference': 'Protein stress marker'
            },
            'amide_III_1221': {
                'target_wavenumber': 1221,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'Amide III (protein structure)',
                'reference': 'Protein structural changes'
            },
            'amide_I_1655': {
                'target_wavenumber': 1655,
                'tolerance': 5,
                'priority': 'CRITICAL',
                'assignment': 'Amide I (α-helix/β-sheet)',
                'reference': 'Protein backbone'
            },

            # Lipid and metabolic markers
            'phospholipid_1285': {
                'target_wavenumber': 1285,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'Phospholipid/cholesterol',
                'reference': 'Membrane changes'
            },
            'lipid_CH2_1440': {
                'target_wavenumber': 1440,
                'tolerance': 5,
                'priority': 'HIGH',
                'assignment': 'CH₂ deformation',
                'reference': 'Membrane/lipid changes'
            },

            # Background/reference bands
            'background_low_600': {
                'target_wavenumber': 600,
                'tolerance': 10,
                'priority': 'LOW',
                'assignment': 'Background reference',
                'reference': 'Data quality marker'
            },
            'background_high_1800': {
                'target_wavenumber': 1800,
                'tolerance': 10,
                'priority': 'LOW',
                'assignment': 'Background reference',
                'reference': 'Data quality marker'
            }
        }

    def _define_ratio_features(self) -> Dict[str, Dict]:
        """
        Define ratio features for batch-invariant classification.
        Easy to add new ratios and modify existing ones.
        """
        return {
            'DNB_ratio_primary': {
                'numerator': 'DNB_primary_1149',
                'denominator': 'DNB_primary_1528',
                'clinical_meaning': 'Primary MGUS/MM classification ratio',
                'priority': 'CRITICAL',
                'expected_direction': 'MGUS_higher'
            },
            'nucleic_protein_ratio': {
                'numerator': ['adenine_726', 'nucleic_backbone_781'],
                'denominator': 'amide_III_1221',
                'clinical_meaning': 'DNA/RNA proliferation vs protein structure',
                'priority': 'HIGH',
                'expected_direction': 'MM_higher'
            },
            'lipid_protein_ratio': {
                'numerator': 'lipid_CH2_1440',
                'denominator': 'amide_I_1655',
                'clinical_meaning': 'Metabolic changes vs protein backbone',
                'priority': 'HIGH',
                'expected_direction': 'MM_higher'
            },
            'metabolic_stress_ratio': {
                'numerator': 'phenylalanine_1004',
                'denominator': 'phospholipid_1285',
                'clinical_meaning': 'Protein stress vs membrane stability',
                'priority': 'HIGH',
                'expected_direction': 'MM_higher'
            },
            'MGUS_progression_marker': {
                'numerator': ['phosphate_1078', 'nucleic_backbone_1190'],
                'denominator': 'amide_I_1655',
                'clinical_meaning': 'Nucleic acid activity progression marker',
                'priority': 'HIGH',
                'expected_direction': 'MM_higher'
            },
            'MM_severity_marker': {
                'numerator': 'nucleic_backbone_786',
                'denominator': 'DNB_primary_1149',
                'clinical_meaning': 'Disease severity indicator',
                'priority': 'HIGH',
                'expected_direction': 'MM_higher'
            }
        }

    def _define_clinical_meanings(self) -> Dict[str, str]:
        """Clinical interpretation of biomarker categories."""
        return {
            'DNB': 'Dynamical Network Biomarker - Primary disease discriminator',
            'nucleic': 'DNA/RNA markers - Cell proliferation and genetic activity',
            'protein': 'Protein structure markers - Cellular stress and function',
            'lipid': 'Lipid/metabolic markers - Membrane and energy metabolism',
            'ratio': 'Batch-invariant clinical ratios - Robust classification features',
            'background': 'Data quality markers - Technical validation',
            'progression': 'Disease progression markers - MGUS→MM transition',
            'severity': 'Disease severity markers - MM staging and prognosis'
        }

    def get_biomarker_subset(self, priority_levels: List[str] = ['HIGH', 'CRITICAL']) -> Dict[str, Dict]:
        """Get subset of biomarkers by priority level."""
        return {
            name: info for name, info in self.biomarker_bands.items()
            if info['priority'] in priority_levels
        }

    def get_ratio_subset(self, priority_levels: List[str] = ['HIGH', 'CRITICAL']) -> Dict[str, Dict]:
        """Get subset of ratios by priority level."""
        return {
            name: info for name, info in self.ratio_definitions.items()
            if info['priority'] in priority_levels
        }

    def validate_ratio_dependencies(self, selected_biomarkers: Dict, selected_ratios: Dict) -> Dict[str, Dict]:
        """Validate that ratio dependencies exist in selected biomarkers."""
        valid_ratios = {}

        for ratio_name, ratio_info in selected_ratios.items():
            # Check numerator dependencies
            num_data = ratio_info['numerator']
            if isinstance(num_data, list):
                missing_num = [
                    b for b in num_data if b not in selected_biomarkers]
            else:
                missing_num = [] if num_data in selected_biomarkers else [num_data]

            # Check denominator dependencies
            den_data = ratio_info['denominator']
            if isinstance(den_data, list):
                missing_den = [
                    b for b in den_data if b not in selected_biomarkers]
            else:
                missing_den = [] if den_data in selected_biomarkers else [den_data]

            if not missing_num and not missing_den:
                valid_ratios[ratio_name] = ratio_info
            else:
                print(
                    f"Skipping ratio '{ratio_name}': missing biomarkers {missing_num + missing_den}")

        return valid_ratios

    def update_biomarker(self, name: str, updates: Dict):
        """Update existing biomarker or add new one."""
        if name in self.biomarker_bands:
            self.biomarker_bands[name].update(updates)
        else:
            self.biomarker_bands[name] = updates

    def update_ratio(self, name: str, updates: Dict):
        """Update existing ratio or add new one."""
        if name in self.ratio_definitions:
            self.ratio_definitions[name].update(updates)
        else:
            self.ratio_definitions[name] = updates


# Global instance for easy import
MGUS_MM_BIOMARKERS = RamanBiomarkerDatabase()

## Biomarker-Enhanced Logistic Regression Model for MGUS/MM Classification ##


class BiomarkerEnhancedLogisticRegressionModel:
    """
    Biomarker-Enhanced Logistic Regression for MGUS/MM Raman Classification.

    Combines the statistical robustness of logistic regression with 
    clinically-validated biomarker features for improved generalization.

    Features:
    - External biomarker database (easy to edit)
    - Logistic regression with proper probability estimation
    - Biomarker feature extraction and validation
    - Clinical interpretation of coefficients
    - Same interface as your existing LogisticRegressionModel
    """

    def __init__(self, data_split: Dict[str, Any],
                 # Logistic Regression parameters
                 penalty: str = 'l2',
                 C: float = 1.0,
                 solver: str = 'lbfgs',
                 max_iter: int = 1000,
                 multi_class: str = 'auto',
                 class_weight: Optional[str] = None,
                 random_state: Optional[int] = 42,
                 # Biomarker parameters
                 biomarker_only: bool = False,
                 biomarker_priority: List[str] = ['CRITICAL', 'HIGH'],
                 scale_features: bool = True,
                 verbose: bool = True,
                 **kwargs):
        """
        Initialize Biomarker-Enhanced Logistic Regression.

        Args:
            data_split: Data dictionary from RamanDataSplitter
            penalty: Regularization ('l1', 'l2', 'elasticnet', 'none')
            C: Inverse regularization strength
            solver: Optimization algorithm
            max_iter: Maximum iterations
            multi_class: Multi-class strategy
            class_weight: Class weights for imbalanced data
            random_state: Random seed
            biomarker_only: Use only biomarker features
            biomarker_priority: Priority levels to include ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
            scale_features: Standardize features
            verbose: Print detailed information
        """
        self.data_split = data_split
        self.biomarker_only = biomarker_only
        self.biomarker_priority = biomarker_priority
        self.scale_features = scale_features
        self.verbose = verbose

        # Check required keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        # Initialize logistic regression
        self.model = LogisticRegression(
            penalty=penalty, C=C, solver=solver, max_iter=max_iter,
            multi_class=multi_class, class_weight=class_weight,
            random_state=random_state, **kwargs
        )

        # Label encoding
        self.label_encoder = LabelEncoder()
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        # Feature scaling
        self.scaler = StandardScaler() if scale_features else None

        # Biomarker components
        self.biomarker_database = MGUS_MM_BIOMARKERS
        self.biomarker_features_train = None
        self.biomarker_features_test = None
        self.biomarker_feature_names = None
        self.biomarker_analysis = None

        if verbose:
            print("=== Biomarker-Enhanced Logistic Regression ===")
            print(f"Classes: {list(self.label_encoder.classes_)}")
            print(
                f"Model: LogisticRegression(C={C}, penalty={penalty}, solver={solver})")
            print(f"Biomarker priority: {biomarker_priority}")
            print(f"Biomarker-only mode: {biomarker_only}")
            print(f"Feature scaling: {scale_features}")

        # Extract biomarker features
        self._extract_biomarker_features()

    def _extract_biomarker_features(self) -> None:
        """Extract biomarker features using external database with proper error handling."""
        wavelengths = self.data_split['unified_wavelengths']

        # Get biomarkers based on priority
        selected_biomarkers = self.biomarker_database.get_biomarker_subset(
            self.biomarker_priority)
        selected_ratios_raw = self.biomarker_database.get_ratio_subset(
            self.biomarker_priority)

        # Validate ratio dependencies
        selected_ratios = self.biomarker_database.validate_ratio_dependencies(
            selected_biomarkers, selected_ratios_raw
        )

        if self.verbose:
            print(f"\n=== Extracting Biomarker Features ===")
            print(f"Selected {len(selected_biomarkers)} individual biomarkers")
            print(
                f"Selected {len(selected_ratios)} valid ratio features (from {len(selected_ratios_raw)} total)")

        # Extract individual biomarker intensities
        biomarker_intensities_train = {}
        biomarker_intensities_test = {}

        for name, info in selected_biomarkers.items():
            target_wn = info['target_wavenumber']
            tolerance = info['tolerance']

            # Find closest wavelength
            idx = np.argmin(np.abs(wavelengths - target_wn))
            actual_wn = wavelengths[idx]

            if abs(actual_wn - target_wn) > tolerance:
                if self.verbose:
                    print(
                        f"Warning: {name} target={target_wn}, actual={actual_wn:.1f} (>{tolerance} cm⁻¹)")

            biomarker_intensities_train[name] = self.data_split['X_train'][:, idx]
            biomarker_intensities_test[name] = self.data_split['X_test'][:, idx]

        # Create ratio features (now validated)
        ratio_features_train = {}
        ratio_features_test = {}

        for ratio_name, ratio_info in selected_ratios.items():
            try:
                # Handle numerator (can be single biomarker or list)
                num_data = ratio_info['numerator']
                if isinstance(num_data, list):
                    # Average multiple biomarkers
                    num_train = np.mean(
                        [biomarker_intensities_train[b] for b in num_data], axis=0)
                    num_test = np.mean([biomarker_intensities_test[b]
                                       for b in num_data], axis=0)
                else:
                    num_train = biomarker_intensities_train[num_data]
                    num_test = biomarker_intensities_test[num_data]

                # Handle denominator
                den_data = ratio_info['denominator']
                if isinstance(den_data, list):
                    den_train = np.mean(
                        [biomarker_intensities_train[b] for b in den_data], axis=0)
                    den_test = np.mean([biomarker_intensities_test[b]
                                       for b in den_data], axis=0)
                else:
                    den_train = biomarker_intensities_train[den_data]
                    den_test = biomarker_intensities_test[den_data]

                # Calculate ratios
                ratio_features_train[ratio_name] = num_train / \
                    (den_train + 1e-8)
                ratio_features_test[ratio_name] = num_test / (den_test + 1e-8)

            except KeyError as e:
                if self.verbose:
                    print(
                        f"Skipping ratio {ratio_name}: missing biomarker {e}")
                continue

        # Combine all features
        all_features_train = {
            **biomarker_intensities_train, **ratio_features_train}
        all_features_test = {
            **biomarker_intensities_test, **ratio_features_test}

        # Convert to arrays
        if all_features_train:  # Check if we have any features
            self.biomarker_features_train = np.column_stack(
                list(all_features_train.values()))
            self.biomarker_features_test = np.column_stack(
                list(all_features_test.values()))
            self.biomarker_feature_names = list(all_features_train.keys())
        else:
            # No biomarker features available
            if self.verbose:
                print("Warning: No biomarker features extracted, using original data")
            self.biomarker_features_train = self.data_split['X_train'].copy()
            self.biomarker_features_test = self.data_split['X_test'].copy()
            self.biomarker_feature_names = [
                f"wn_{wn:.1f}" for wn in wavelengths]
            self.biomarker_only = False  # Force to include original features

        # Combine with original features if not biomarker-only
        if not self.biomarker_only:
            self.biomarker_features_train = np.column_stack([
                self.data_split['X_train'],
                self.biomarker_features_train
            ])
            self.biomarker_features_test = np.column_stack([
                self.data_split['X_test'],
                self.biomarker_features_test
            ])
            # Add wavelength names
            wn_names = [f"wn_{wn:.1f}" for wn in wavelengths]
            self.biomarker_feature_names = wn_names + self.biomarker_feature_names

        if self.verbose:
            print(
                f"Final feature matrix: {self.biomarker_features_train.shape}")
            print(f"Biomarker features: {len(all_features_train)}")
            if not self.biomarker_only:
                print(f"Original features: {len(wavelengths)}")

    def _extract_biomarker_features_from_external(self, X_external: np.ndarray) -> np.ndarray:
        """Extract biomarker features from external data."""
        wavelengths = self.data_split['unified_wavelengths']

        selected_biomarkers = self.biomarker_database.get_biomarker_subset(
            self.biomarker_priority)
        selected_ratios_raw = self.biomarker_database.get_ratio_subset(
            self.biomarker_priority)
        selected_ratios = self.biomarker_database.validate_ratio_dependencies(
            selected_biomarkers, selected_ratios_raw
        )

        # Extract individual biomarkers
        biomarker_intensities = {}
        for name, info in selected_biomarkers.items():
            idx = np.argmin(np.abs(wavelengths - info['target_wavenumber']))
            biomarker_intensities[name] = X_external[:, idx]

        # Create ratios
        ratio_features = {}
        for ratio_name, ratio_info in selected_ratios.items():
            try:
                # Numerator
                num_data = ratio_info['numerator']
                if isinstance(num_data, list):
                    num_vals = np.mean([biomarker_intensities[b]
                                       for b in num_data], axis=0)
                else:
                    num_vals = biomarker_intensities[num_data]

                # Denominator
                den_data = ratio_info['denominator']
                if isinstance(den_data, list):
                    den_vals = np.mean([biomarker_intensities[b]
                                       for b in den_data], axis=0)
                else:
                    den_vals = biomarker_intensities[den_data]

                ratio_features[ratio_name] = num_vals / (den_vals + 1e-8)
            except KeyError:
                continue

        # Combine features
        all_features = {**biomarker_intensities, **ratio_features}

        if all_features:
            biomarker_features_external = np.column_stack(
                list(all_features.values()))
        else:
            biomarker_features_external = np.empty((X_external.shape[0], 0))

        # Add original features if not biomarker-only
        if not self.biomarker_only:
            biomarker_features_external = np.column_stack(
                [X_external, biomarker_features_external])

        return biomarker_features_external

    def fit(self) -> None:
        """Fit the biomarker-enhanced logistic regression model."""
        if self.verbose:
            print("\n=== Fitting Biomarker-Enhanced Logistic Regression ===")

        # Prepare features
        X_train = self.biomarker_features_train.copy()

        # Apply scaling
        if self.scaler is not None:
            X_train = self.scaler.fit_transform(X_train)

        # Fit model
        try:
            self.model.fit(X_train, self.y_train_encoded)
            if self.verbose:
                print("Model fitted successfully on biomarker-enhanced features")
                if hasattr(self.model, 'n_iter_'):
                    print(f"Convergence: {self.model.n_iter_} iterations")
        except Exception as e:
            print(f"Fitting error: {e}")
            raise e

        # Analyze biomarker coefficients
        self._analyze_biomarker_coefficients()

    def _analyze_biomarker_coefficients(self) -> None:
        """Analyze logistic regression coefficients for biomarker interpretation."""
        if not hasattr(self.model, 'coef_'):
            return

        coefficients = self.model.coef_[0] if len(
            self.label_encoder.classes_) == 2 else self.model.coef_

        # Create biomarker analysis
        biomarker_analysis = []

        for i, (name, coef) in enumerate(zip(self.biomarker_feature_names, coefficients)):
            # Get clinical meaning
            clinical_meaning = "Unknown"
            for category, meaning in self.biomarker_database.clinical_interpretations.items():
                if category.lower() in name.lower():
                    clinical_meaning = meaning
                    break

            # Determine effect direction for binary classification
            if len(self.label_encoder.classes_) == 2:
                if coef > 0:
                    effect = f"Favors {self.label_encoder.classes_[1]}"
                else:
                    effect = f"Favors {self.label_encoder.classes_[0]}"
            else:
                effect = "Multi-class coefficient"

            biomarker_analysis.append({
                'feature': name,
                'coefficient': coef,
                'abs_coefficient': abs(coef),
                'clinical_meaning': clinical_meaning,
                'effect_direction': effect
            })

        # Sort by importance
        biomarker_analysis.sort(
            key=lambda x: x['abs_coefficient'], reverse=True)
        self.biomarker_analysis = biomarker_analysis

        if self.verbose:
            print(f"\n=== Top 10 Biomarker Coefficients ===")
            for i, analysis in enumerate(biomarker_analysis[:10]):
                print(f"{i+1:2d}. {analysis['feature'][:20]:20s} | "
                      f"Coef: {analysis['coefficient']:8.4f} | "
                      f"{analysis['effect_direction']}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using biomarker features."""
        biomarker_features = self._extract_biomarker_features_from_external(X)
        if self.scaler is not None:
            biomarker_features = self.scaler.transform(biomarker_features)
        return self.model.predict(biomarker_features)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """Predict and decode to original labels."""
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predict class probabilities using biomarker features."""
        biomarker_features = self._extract_biomarker_features_from_external(X)
        if self.scaler is not None:
            biomarker_features = self.scaler.transform(biomarker_features)
        return self.model.predict_proba(biomarker_features)

    def get_feature_importance(self, top_n: int = 20) -> Dict[str, Any]:
        """Get biomarker feature importance based on logistic regression coefficients."""
        if not hasattr(self.model, 'coef_'):
            raise ValueError(
                "Model must be fitted before getting feature importance")

        coefficients = self.model.coef_

        if len(self.label_encoder.classes_) == 2:
            # Binary classification
            coef_abs = np.abs(coefficients[0])
            feature_importance = [
                {
                    'wavelength': self.biomarker_feature_names[i].replace('wn_', '').replace('_', ' '),
                    'coefficient': coefficients[0][i],
                    'abs_coefficient': coef_abs[i],
                    'rank': rank,
                    'clinical_meaning': self.biomarker_analysis[i]['clinical_meaning'] if self.biomarker_analysis else 'Unknown'
                }
                for rank, i in enumerate(np.argsort(coef_abs)[::-1], 1)
            ]
        else:
            # Multi-class classification
            coef_abs = np.mean(np.abs(coefficients), axis=0)
            feature_importance = [
                {
                    'wavelength': self.biomarker_feature_names[i].replace('wn_', '').replace('_', ' '),
                    'coefficients': {class_name: coefficients[j][i]
                                     for j, class_name in enumerate(self.label_encoder.classes_)},
                    'avg_abs_coefficient': coef_abs[i],
                    'rank': rank,
                    'clinical_meaning': self.biomarker_analysis[i]['clinical_meaning'] if self.biomarker_analysis else 'Unknown'
                }
                for rank, i in enumerate(np.argsort(coef_abs)[::-1], 1)
            ]

        return {
            'feature_importance': feature_importance[:top_n],
            'n_features': len(feature_importance),
            'is_binary': len(self.label_encoder.classes_) == 2,
            'biomarker_enhanced': True
        }

    def plot_feature_importance(self, top_n: int = 20) -> None:
        """Plot biomarker feature importance with clinical annotations."""
        importance_data = self.get_feature_importance(top_n)
        features = importance_data['feature_importance']

        if importance_data['is_binary']:
            # Binary classification plot
            wavelengths = [f['wavelength'] for f in features]
            coefficients = [f['coefficient'] for f in features]

            plt.figure(figsize=(14, 8))
            colors = ['red' if c < 0 else 'blue' for c in coefficients]

            bars = plt.barh(range(len(wavelengths)),
                            coefficients, color=colors, alpha=0.7)
            plt.yticks(range(len(wavelengths)), [f"{w}" for w in wavelengths])
            plt.xlabel('Logistic Regression Coefficient')
            plt.title(
                f'Top {top_n} Most Important Biomarker Features (Binary Classification)')
            plt.grid(True, alpha=0.3, axis='x')

            # Legend
            import matplotlib.patches as mpatches
            red_patch = mpatches.Patch(color='red', alpha=0.7,
                                       label=f'Favors {self.label_encoder.classes_[0]}')
            blue_patch = mpatches.Patch(color='blue', alpha=0.7,
                                        label=f'Favors {self.label_encoder.classes_[1]}')
            plt.legend(handles=[red_patch, blue_patch])

        else:
            # Multi-class plot
            wavelengths = [f['wavelength'] for f in features]
            avg_coeffs = [f['avg_abs_coefficient'] for f in features]

            plt.figure(figsize=(14, 8))
            bars = plt.barh(range(len(wavelengths)),
                            avg_coeffs, color='green', alpha=0.7)
            plt.yticks(range(len(wavelengths)), [f"{w}" for w in wavelengths])
            plt.xlabel('Average Absolute Coefficient')
            plt.title(
                f'Top {top_n} Most Important Biomarker Features (Multi-class)')
            plt.grid(True, alpha=0.3, axis='x')

        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

    def evaluate(self) -> Dict[str, Any]:
        """Evaluate model with same interface as original LogisticRegressionModel."""
        # Prepare test data with biomarker features
        X_test = self.biomarker_features_test
        if self.scaler is not None:
            X_test = self.scaler.transform(X_test)

        # Get predictions
        y_pred_encoded = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)
        y_pred_labels = self.label_encoder.inverse_transform(y_pred_encoded)
        y_true_labels = self.data_split['y_test']

        # Calculate metrics (same as original)
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        # Logistic regression specific metrics
        log_loss_score = log_loss(self.y_test_encoded, y_pred_proba)

        try:
            if len(self.label_encoder.classes_) == 2:
                auc_roc = roc_auc_score(
                    self.y_test_encoded, y_pred_proba[:, 1])
            else:
                auc_roc = roc_auc_score(
                    self.y_test_encoded, y_pred_proba, multi_class='ovr', average='weighted')
        except ValueError:
            auc_roc = None

        # Compatibility metrics
        if len(self.label_encoder.classes_) == 2:
            continuous_predictions = y_pred_proba[:, 1]
            continuous_targets = self.y_test_encoded.astype(float)
        else:
            continuous_predictions = np.max(y_pred_proba, axis=1)
            continuous_targets = self.y_test_encoded.astype(
                float) / (len(self.label_encoder.classes_) - 1)

        mse = mean_squared_error(continuous_targets, continuous_predictions)
        r2 = r2_score(continuous_targets, continuous_predictions)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2,
                'note': 'Compatibility metrics from probabilities'
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            },
            'logistic_regression': {
                'log_loss': log_loss_score,
                'auc_roc': auc_roc
            },
            'biomarker_enhanced': True
        }

        if self.verbose:
            print("=== Biomarker-Enhanced Logistic Regression Results ===")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"Log Loss: {log_loss_score:.4f}")
            if auc_roc:
                print(f"AUC-ROC: {auc_roc:.4f}")
            print(f"F1-weighted: {f1_weighted:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true_labels, y_pred_labels,
                                        target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """Plot confusion matrix (same interface as original)."""
        # Prepare test data
        X_test = self.biomarker_features_test
        if self.scaler is not None:
            X_test = self.scaler.transform(X_test)

        y_pred_labels = self.label_encoder.inverse_transform(
            self.model.predict(X_test))
        y_true = self.data_split['y_test']

        # Compute confusion matrix
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nBiomarker-Enhanced Logistic Regression Confusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Biomarker-Enhanced Logistic Regression Confusion Matrix')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Biomarker-Enhanced Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None,
                         show_confusion_matrix: bool = False, show_probabilities: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.
        Same interface as original LogisticRegressionModel.
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        if show_probabilities:
            result['prediction_probabilities'] = self.predict_proba(X_new)

        print(f"=== Biomarker-Enhanced Logistic Regression Predictions ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Calculate metrics
            y_new_encoded = self.label_encoder.transform(y_new)
            y_pred_proba = self.predict_proba(X_new)

            # Compatibility metrics
            if len(self.label_encoder.classes_) == 2:
                continuous_predictions = y_pred_proba[:, 1]
                continuous_targets = y_new_encoded.astype(float)
            else:
                continuous_predictions = np.max(y_pred_proba, axis=1)
                continuous_targets = y_new_encoded.astype(
                    float) / (len(self.label_encoder.classes_) - 1)

            mse = mean_squared_error(
                continuous_targets, continuous_predictions)
            r2 = r2_score(continuous_targets, continuous_predictions)

            # Classification metrics
            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"F1-weighted: {f1_weighted:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """Plot confusion matrix for new data predictions."""
        # Same implementation as original LogisticRegressionModel
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Biomarker-Enhanced Confusion Matrix for New Data')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title(
                'Biomarker-Enhanced Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """Save the trained model and components."""
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'scaler': self.scaler,
            'data_split': self.data_split,
            'biomarker_feature_names': self.biomarker_feature_names,
            'biomarker_analysis': self.biomarker_analysis,
            'biomarker_priority': self.biomarker_priority,
            'biomarker_only': self.biomarker_only,
            'scale_features': self.scale_features
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Biomarker-Enhanced Logistic Regression saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'BiomarkerEnhancedLogisticRegressionModel':
        """Load a trained model from file."""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = BiomarkerEnhancedLogisticRegressionModel(
            model_data['data_split'],
            biomarker_priority=model_data.get(
                'biomarker_priority', ['CRITICAL', 'HIGH']),
            biomarker_only=model_data.get('biomarker_only', False),
            scale_features=model_data.get('scale_features', True),
            verbose=False
        )

        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        instance.scaler = model_data.get('scaler')
        instance.biomarker_feature_names = model_data.get(
            'biomarker_feature_names')
        instance.biomarker_analysis = model_data.get('biomarker_analysis')

        return instance

# K-Nearest Neighbors (KNN) Model


class KNNModel:
    """
    A class for training and evaluating a K-Nearest Neighbors (KNN) model on Raman spectroscopy data.

    This model is designed for classification tasks with categorical labels (e.g., 'MGUS', 'MM').
    It encodes labels numerically for compatibility with sklearn's KNeighborsClassifier.
    """

    def __init__(self, data_split: Dict[str, Any], n_neighbors: int = 5, **kwargs):
        """
        Initialize the KNN model.

        Args:
            data_split (dict): Data dictionary from RamanDataPreparer.prepare_data()
            n_neighbors (int): Number of neighbors to use for KNN (default 5)
            **kwargs: Additional keyword arguments for KNeighborsClassifier
        """
        self.data_split = data_split
        self.n_neighbors = n_neighbors

        # Check for missing keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        self.model = KNeighborsClassifier(
            n_neighbors=self.n_neighbors, **kwargs)
        self.label_encoder = LabelEncoder()

        # Encode labels numerically for classification
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")
        print(f"KNN model initialized with n_neighbors={self.n_neighbors}")

    def fit(self) -> None:
        """
        Fit the KNN model to the training data.
        """
        print("Fitting KNN model...")
        self.model.fit(self.data_split['X_train'], self.y_train_encoded)
        print("Model fitted successfully.")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        return self.model.predict(X)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None, show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print("\nRegression Metrics:")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.

        Args:
            y_true (np.ndarray): True labels
            y_pred (List[str]): Predicted labels
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix for New Data (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.

        Returns:
            Dict[str, Any]: Evaluation metrics including classification metrics
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Note: KNN is a classification model, so regression metrics are not applicable.
        # We include dummy values for consistency with the LinearRegressionModel format.
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        print("=== Detailed Evaluation Metrics ===")
        print("\nRegression Metrics (Note: Not applicable for KNN classification model):")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  R² Score: {r2:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix using sklearn's official implementation.

        Args:
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                               for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Confusion Matrix (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and encoder to a pickle file.

        Args:
            filepath (str): Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split,
            'n_neighbors': self.n_neighbors
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'KNNModel':
        """
        Load a trained model from a pickle file.

        Args:
            filepath (str): Path to the saved model file

        Returns:
            KNNModel: Loaded model instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = KNNModel(
            model_data['data_split'], n_neighbors=model_data.get('n_neighbors', 5))
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        return instance

# Random Forest Model


class RandomForestModel:
    """
    A class for training and evaluating a Random Forest model on Raman spectroscopy data.

    This model is designed for classification tasks with categorical labels (e.g., 'MGUS', 'MM').
    It encodes labels numerically for compatibility with sklearn's RandomForestClassifier.
    """

    def __init__(self, data_split: Dict[str, Any], n_estimators: int = 100, **kwargs):
        """
        Initialize the Random Forest model.

        Args:
            data_split (dict): Data dictionary from RamanDataPreparer.prepare_data()
            n_estimators (int): Number of trees in the forest (default 100)
            **kwargs: Additional keyword arguments for RandomForestClassifier
        """
        self.data_split = data_split
        self.n_estimators = n_estimators

        # Check for missing keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        self.model = RandomForestClassifier(
            n_estimators=self.n_estimators, **kwargs)
        self.label_encoder = LabelEncoder()

        # Encode labels numerically for classification
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")
        print(
            f"Random Forest model initialized with n_estimators={self.n_estimators}")

    def fit(self) -> None:
        """
        Fit the Random Forest model to the training data.
        """
        print("Fitting Random Forest model...")
        self.model.fit(self.data_split['X_train'], self.y_train_encoded)
        print("Model fitted successfully.")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        return self.model.predict(X)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None, show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print(
                "\nRegression Metrics (Note: Not applicable for SVM classification model):")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.

        Args:
            y_true (np.ndarray): True labels
            y_pred (List[str]): Predicted labels
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix for New Data (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix using sklearn's official implementation.

        Args:
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Confusion Matrix (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and encoder to a pickle file.

        Args:
            filepath (str): Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split,
            'n_estimators': self.n_estimators
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filepath}")

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.

        Returns:
            Dict[str, Any]: Evaluation metrics including classification metrics
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Note: Random Forest is a classification model, so regression metrics are not applicable.
        # We include dummy values for consistency with other model formats.
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        print("=== Detailed Evaluation Metrics ===")
        print("\nRegression Metrics (Note: Not applicable for Random Forest classification model):")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  R² Score: {r2:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    @staticmethod
    def load_model(filepath: str) -> 'RandomForestModel':
        """
        Load a trained model from a pickle file.

        Args:
            filepath (str): Path to the saved model file

        Returns:
            RandomForestModel: Loaded model instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = RandomForestModel(
            model_data['data_split'], n_estimators=model_data.get('n_estimators', 100))
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        return instance

# Support Vector Machine (SVM) Model


class SVMModel:
    """
    A class for training and evaluating a Support Vector Machine (SVM) model on Raman spectroscopy data.

    This model is designed for classification tasks with categorical labels (e.g., 'MGUS', 'MM').
    It encodes labels numerically for compatibility with sklearn's SVC.
    """

    def __init__(self, data_split: Dict[str, Any], C: float = 1.0, kernel: str = 'rbf', **kwargs):
        """
        Initialize the SVM model.

        Args:
            data_split (dict): Data dictionary from RamanDataPreparer.prepare_data()
            C (float): Regularization parameter (default 1.0)
            kernel (str): Kernel type ('linear', 'poly', 'rbf', 'sigmoid', default 'rbf')
            **kwargs: Additional keyword arguments for SVC
        """
        self.data_split = data_split
        self.C = C
        self.kernel = kernel

        # Check for missing keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        self.model = SVC(C=self.C, kernel=self.kernel, **kwargs)
        self.label_encoder = LabelEncoder()

        # Encode labels numerically for classification
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")
        print(f"SVM model initialized with C={self.C}, kernel={self.kernel}")

    def fit(self) -> None:
        """
        Fit the SVM model to the training data.
        """
        print("Fitting SVM model...")
        self.model.fit(self.data_split['X_train'], self.y_train_encoded)
        print("Model fitted successfully.")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        return self.model.predict(X)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None, show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print(
                "\nRegression Metrics (Note: Not applicable for SVM classification model):")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.

        Args:
            y_true (np.ndarray): True labels
            y_pred (List[str]): Predicted labels
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'Confusion Matrix for New Data (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix using sklearn's official implementation.

        Args:
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Confusion Matrix (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.

        Returns:
            Dict[str, Any]: Evaluation metrics including classification metrics
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Note: SVM is a classification model, so regression metrics are not applicable.
        # We include dummy values for consistency with other model formats.
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        print("=== Detailed Evaluation Metrics ===")
        print("\nRegression Metrics (Note: Not applicable for SVM classification model):")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  R² Score: {r2:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and encoder to a pickle file.

        Args:
            filepath (str): Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split,
            'C': self.C,
            'kernel': self.kernel
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'SVMModel':
        """
        Load a trained model from a pickle file.

        Args:
            filepath (str): Path to the saved model file

        Returns:
            SVMModel: Loaded model instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = SVMModel(model_data['data_split'], C=model_data.get(
            'C', 1.0), kernel=model_data.get('kernel', 'rbf'))
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        return instance

# XGBoost Model


class XGBoostModel:
    """
    A class for training and evaluating an XGBoost model on Raman spectroscopy data.

    XGBoost (eXtreme Gradient Boosting) is a powerful ensemble learning algorithm that builds
    multiple decision trees sequentially, where each tree corrects the errors of previous trees.
    It's particularly effective for structured/tabular data like Raman spectroscopy features.

    Key advantages:
    - High predictive performance on structured data
    - Built-in handling of missing values
    - Feature importance ranking
    - Regularization to prevent overfitting
    - Efficient parallel processing
    """

    def __init__(self, data_split: Dict[str, Any],
                 n_estimators: int = 100,
                 max_depth: int = 6,
                 learning_rate: float = 0.3,
                 subsample: float = 1.0,
                 colsample_bytree: float = 1.0,
                 reg_alpha: float = 0,
                 reg_lambda: float = 1,
                 random_state: int = 42,
                 **kwargs):
        """
        Initialize the XGBoost model.

        Args:
            data_split (dict): Data dictionary from RamanDataPreparer.prepare_data()
            n_estimators (int): Number of boosting rounds (trees), default 100
            max_depth (int): Maximum depth of trees, default 6
            learning_rate (float): Step size shrinkage to prevent overfitting, default 0.3
            subsample (float): Fraction of samples used for each tree, default 1.0
            colsample_bytree (float): Fraction of features used for each tree, default 1.0
            reg_alpha (float): L1 regularization term, default 0
            reg_lambda (float): L2 regularization term, default 1
            random_state (int): Random seed for reproducibility, default 42
            **kwargs: Additional keyword arguments for XGBClassifier
        """
        self.data_split = data_split
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.learning_rate = learning_rate
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.reg_alpha = reg_alpha
        self.reg_lambda = reg_lambda
        self.random_state = random_state

        # Check for missing keys
        required_keys = ['X_train', 'X_test',
                         'y_train', 'y_test', 'unified_wavelengths']
        missing_keys = [
            key for key in required_keys if key not in self.data_split]
        if missing_keys:
            raise ValueError(f"Missing keys in data_split: {missing_keys}")

        # Initialize XGBoost classifier
        xgb_params = {
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'learning_rate': self.learning_rate,
            'subsample': self.subsample,
            'colsample_bytree': self.colsample_bytree,
            'reg_alpha': self.reg_alpha,
            'reg_lambda': self.reg_lambda,
            'random_state': self.random_state,
            'eval_metric': 'logloss',  # Use logloss for binary classification
            # Explicitly set objective for binary classification
            'objective': 'binary:logistic',
        }

        # Add any additional parameters
        xgb_params.update(kwargs)

        self.model = xgb.XGBClassifier(**xgb_params)

        self.label_encoder = LabelEncoder()

        # Encode labels numerically for classification
        self.y_train_encoded = self.label_encoder.fit_transform(
            self.data_split['y_train'])
        self.y_test_encoded = self.label_encoder.transform(
            self.data_split['y_test'])

        print(
            f"Encoded labels: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}")
        print(
            f"XGBoost model initialized with n_estimators={self.n_estimators}, max_depth={self.max_depth}, learning_rate={self.learning_rate}")

    def fit(self, early_stopping_rounds: Optional[int] = None, verbose: bool = True) -> None:
        """
        Fit the XGBoost model to the training data.

        Args:
            early_stopping_rounds (int, optional): Number of rounds to wait for improvement before stopping
            verbose (bool): Whether to print training progress
        """
        print("Fitting XGBoost model...")

        if early_stopping_rounds is not None:
            # Use early stopping with validation set - newer XGBoost API
            try:
                self.model.fit(
                    self.data_split['X_train'],
                    self.y_train_encoded,
                    eval_set=[
                        (self.data_split['X_test'], self.y_test_encoded)],
                    verbose=verbose if verbose else 0
                )
                print("Model fitted with validation monitoring")
            except Exception as e:
                print(
                    f"Early stopping failed ({e}), falling back to standard training...")
                # Fallback to standard training
                self.model.fit(
                    self.data_split['X_train'], self.y_train_encoded)
                print("Model fitted successfully (standard training)")
        else:
            # Standard fitting without early stopping
            self.model.fit(self.data_split['X_train'], self.y_train_encoded)
            print("Model fitted successfully.")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions on new data.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        return self.model.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class probabilities.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted class probabilities
        """
        return self.model.predict_proba(X)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def get_feature_importance(self, importance_type: str = 'weight') -> Dict[str, float]:
        """
        Get feature importance scores from the trained model.

        Args:
            importance_type (str): Type of importance ('weight', 'gain', 'cover')

        Returns:
            Dict[str, float]: Dictionary mapping wavelengths to importance scores
        """
        if not hasattr(self.model, 'feature_importances_'):
            raise ValueError(
                "Model must be fitted before getting feature importance")

        # Get feature importances using the standard sklearn interface
        importance_values = self.model.feature_importances_

        # Map feature indices to wavelengths
        wavelengths = self.data_split['unified_wavelengths']
        feature_importance = {}

        for i, wavelength in enumerate(wavelengths):
            if i < len(importance_values):
                feature_importance[f'{wavelength:.2f}'] = importance_values[i]
            else:
                feature_importance[f'{wavelength:.2f}'] = 0.0

        return feature_importance

    def plot_feature_importance(self, top_n: int = 20, importance_type: str = 'weight') -> None:
        """
        Plot the top N most important features.

        Args:
            top_n (int): Number of top features to plot
            importance_type (str): Type of importance (for compatibility, uses sklearn interface)
        """
        feature_importance = self.get_feature_importance(importance_type)

        # Sort by importance and get top N
        sorted_features = sorted(feature_importance.items(
        ), key=lambda x: x[1], reverse=True)[:top_n]

        if not sorted_features:
            print("No feature importance data available")
            return

        wavelengths, importances = zip(*sorted_features)

        plt.figure(figsize=(12, 8))
        plt.barh(range(len(wavelengths)), importances)
        plt.yticks(range(len(wavelengths)), [f'{w} cm⁻¹' for w in wavelengths])
        plt.xlabel(f'Feature Importance')
        plt.ylabel('Wavelength')
        plt.title(f'Top {top_n} Most Important Features (XGBoost)')
        plt.gca().invert_yaxis()  # Highest importance at top
        plt.tight_layout()
        plt.show()

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None,
                         show_confusion_matrix: bool = False, show_probabilities: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided
            show_probabilities (bool): Whether to include prediction probabilities

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_encoded = self.predict(X_new)
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_encoded': predictions_encoded,
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        if show_probabilities:
            result['prediction_probabilities'] = self.predict_proba(X_new)

        print(f"=== XGBoost Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions if true labels are provided
            mse = mean_squared_error(
                self.label_encoder.transform(y_new), predictions_encoded)
            r2 = r2_score(self.label_encoder.transform(
                y_new), predictions_encoded)

            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'regression': {
                    'mean_squared_error': mse,
                    'r2_score': r2
                },
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print(
                "\nRegression Metrics (Note: Not applicable for XGBoost classification model):")
            print(f"  Mean Squared Error: {mse:.4f}")
            print(f"  R² Score: {r2:.4f}")

            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def evaluate(self) -> Dict[str, Any]:
        """
        Evaluate the model on the test set with detailed classification metrics.

        Returns:
            Dict[str, Any]: Evaluation metrics including classification metrics
        """
        y_pred_encoded = self.predict(self.data_split['X_test'])
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true_labels = self.data_split['y_test']

        # Note: XGBoost is used as a classification model here, so regression metrics are for consistency.
        mse = mean_squared_error(self.y_test_encoded, y_pred_encoded)
        r2 = r2_score(self.y_test_encoded, y_pred_encoded)

        # Classification metrics
        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        precision_macro = precision_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_true_labels, y_pred_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_true_labels, y_pred_labels,
                            average='macro', zero_division=0)

        precision_weighted = precision_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        recall_weighted = recall_score(
            y_true_labels, y_pred_labels, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true_labels, y_pred_labels,
                               average='weighted', zero_division=0)

        # Per-class metrics
        precision_per_class = precision_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        recall_per_class = recall_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)
        f1_per_class = f1_score(
            y_true_labels, y_pred_labels, average=None, zero_division=0)

        metrics = {
            'regression': {
                'mean_squared_error': mse,
                'r2_score': r2
            },
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro,
                'precision_weighted': precision_weighted,
                'recall_weighted': recall_weighted,
                'f1_weighted': f1_weighted,
                'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
            }
        }

        print("=== XGBoost Detailed Evaluation Metrics ===")
        print(
            "\nRegression Metrics (Note: Not applicable for XGBoost classification model):")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  R² Score: {r2:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true_labels, y_pred_labels,
              target_names=self.label_encoder.classes_, zero_division=0))

        return metrics

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.

        Args:
            y_true (np.ndarray): True labels
            y_pred (List[str]): Predicted labels
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                             for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title(
                'XGBoost Confusion Matrix for New Data (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('XGBoost Classification Report for New Data')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def plot_confusion_matrix(self, show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix using sklearn's official implementation.

        Args:
            show_plot (bool): Whether to display the plot

        Returns:
            np.ndarray: Confusion matrix
        """
        y_pred_labels = self.predict_labels(self.data_split['X_test'])
        y_true = self.data_split['y_test']

        # Compute confusion matrix with original labels using sklearn
        cm = confusion_matrix(y_true, y_pred_labels,
                              labels=self.label_encoder.classes_)

        # Print confusion matrix as text
        print("\nXGBoost Confusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            # Generate classification report
            report = classification_report(
                y_true, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            # Create figure with subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Calculate total for percentages
            total = cm.sum()

            # Create annotation array with counts and percentages
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            # Plot heatmap on ax1
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('XGBoost Confusion Matrix (Counts and Percentages)')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            # Plot classification report on ax2
            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('XGBoost Classification Report')
            ax2.axis('off')  # Hide axes for text

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """
        Save the trained model and encoder to a pickle file.

        Args:
            filepath (str): Path to save the file
        """
        model_data = {
            'model': self.model,
            'label_encoder': self.label_encoder,
            'data_split': self.data_split,
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'learning_rate': self.learning_rate,
            'subsample': self.subsample,
            'colsample_bytree': self.colsample_bytree,
            'reg_alpha': self.reg_alpha,
            'reg_lambda': self.reg_lambda,
            'random_state': self.random_state
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"XGBoost model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'XGBoostModel':
        """
        Load a trained model from a pickle file.

        Args:
            filepath (str): Path to the saved model file

        Returns:
            XGBoostModel: Loaded model instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = XGBoostModel(
            model_data['data_split'],
            n_estimators=model_data.get('n_estimators', 100),
            max_depth=model_data.get('max_depth', 6),
            learning_rate=model_data.get('learning_rate', 0.3),
            subsample=model_data.get('subsample', 1.0),
            colsample_bytree=model_data.get('colsample_bytree', 1.0),
            reg_alpha=model_data.get('reg_alpha', 0),
            reg_lambda=model_data.get('reg_lambda', 1),
            random_state=model_data.get('random_state', 42)
        )
        instance.model = model_data['model']
        instance.label_encoder = model_data['label_encoder']
        return instance


# Ensemble Voting Classifier
class EnsembleVotingClassifier(BaseEstimator, ClassifierMixin):
    """
    A custom ensemble classifier that combines predictions from multiple pre-trained models
    using majority voting (hard voting) or probability averaging (soft voting).

    This class wraps sklearn's VotingClassifier but is designed to work with your custom
    model classes (LinearRegressionModel, KNNModel, etc.) that have already been trained.
    """

    def __init__(self, models: List[Any], voting: str = 'hard', weights: Optional[List[float]] = None):
        """
        Initialize the ensemble classifier.

        Args:
            models (List[Any]): List of pre-trained model instances (e.g., [lr_model, knn_model, ...])
            voting (str): 'hard' for majority voting, 'soft' for probability averaging (default 'hard')
            weights (List[float], optional): Weights for each model in voting (default None for equal weights)
        """
        self.models = models
        self.voting = voting
        self.weights = weights or [1.0] * len(models)

        if len(self.weights) != len(self.models):
            raise ValueError("Number of weights must match number of models")

        # Infer label encoder from the first model (assuming all use the same encoding)
        if self.models:
            self.label_encoder = self.models[0].label_encoder
            self.classes_ = self.label_encoder.classes_
        else:
            raise ValueError("At least one model must be provided")

        # Check if all models support soft voting
        if self.voting == 'soft':
            for i, model in enumerate(self.models):
                if not hasattr(model, 'predict_proba'):
                    print(
                        f"Warning: Model {i} ({type(model).__name__}) does not have predict_proba method. Falling back to hard voting.")
                    self.voting = 'hard'
                    break

        print(
            f"EnsembleVotingClassifier initialized with {len(self.models)} models using {self.voting} voting")
        print(f"Models: {[type(model).__name__ for model in self.models]}")

    def fit(self, X=None, y=None):
        """
        Dummy fit method for sklearn compatibility. Since models are pre-trained, this does nothing.
        """
        # Models are already trained, so no fitting needed
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions using ensemble voting.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted encoded labels
        """
        if self.voting == 'hard':
            return self._predict_hard(X)
        else:
            return self._predict_soft(X)

    def _predict_hard(self, X: np.ndarray) -> np.ndarray:
        """
        Hard voting: majority vote on predicted labels.
        """
        predictions = []
        for model in self.models:
            pred_labels = model.predict_labels(X)
            pred_encoded = self.label_encoder.transform(pred_labels)
            predictions.append(pred_encoded)

        # Stack predictions and find majority vote
        predictions = np.array(predictions)  # Shape: (n_models, n_samples)

        # For each sample, find the most common prediction
        majority_predictions = []
        for i in range(predictions.shape[1]):
            sample_preds = predictions[:, i]
            unique, counts = np.unique(sample_preds, return_counts=True)
            majority_pred = unique[np.argmax(counts)]
            majority_predictions.append(majority_pred)

        return np.array(majority_predictions)

    def _predict_soft(self, X: np.ndarray) -> np.ndarray:
        """
        Soft voting: average probabilities and choose highest.
        """
        probabilities = []
        for model in self.models:
            if hasattr(model, 'predict_proba'):
                prob = model.predict_proba(X)
                probabilities.append(prob)
            else:
                # Fallback to hard voting for this model
                pred_labels = model.predict_labels(X)
                pred_encoded = self.label_encoder.transform(pred_labels)
                # Convert to one-hot probabilities
                prob = np.zeros((len(pred_encoded), len(self.classes_)))
                for i, pred in enumerate(pred_encoded):
                    prob[i, pred] = 1.0
                probabilities.append(prob)

        # Average probabilities (weighted if weights provided)
        avg_probabilities = np.average(
            probabilities, axis=0, weights=self.weights)

        # Choose class with highest average probability
        return np.argmax(avg_probabilities, axis=1)

    def predict_labels(self, X: np.ndarray) -> List[str]:
        """
        Make predictions and decode to original labels.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            List[str]: Predicted original labels
        """
        predictions_encoded = self.predict(X)
        return self.label_encoder.inverse_transform(predictions_encoded)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class probabilities using soft voting.

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Predicted class probabilities
        """
        if self.voting == 'soft':
            probabilities = []
            for model in self.models:
                if hasattr(model, 'predict_proba'):
                    prob = model.predict_proba(X)
                    probabilities.append(prob)
                else:
                    # Fallback: convert hard predictions to probabilities
                    pred_labels = model.predict_labels(X)
                    pred_encoded = self.label_encoder.transform(pred_labels)
                    prob = np.zeros((len(pred_encoded), len(self.classes_)))
                    for i, pred in enumerate(pred_encoded):
                        prob[i, pred] = 1.0
                    probabilities.append(prob)

            # Average probabilities
            return np.average(probabilities, axis=0, weights=self.weights)
        else:
            # For hard voting, convert predictions to probabilities
            predictions = self.predict(X)
            probabilities = np.zeros((len(predictions), len(self.classes_)))
            for i, pred in enumerate(predictions):
                probabilities[i, pred] = 1.0
            return probabilities

    def predict_new_data(self, X_new: np.ndarray, y_new: Optional[np.ndarray] = None,
                         show_confusion_matrix: bool = False) -> Dict[str, Any]:
        """
        Predict on new data and optionally evaluate if true labels are provided.

        Args:
            X_new (np.ndarray): New feature matrix for prediction
            y_new (np.ndarray, optional): True labels for new data (if available)
            show_confusion_matrix (bool): Whether to display the confusion matrix if y_new is provided

        Returns:
            Dict[str, Any]: Dictionary containing predictions and evaluation metrics (if y_new provided)
        """
        predictions_labels = self.predict_labels(X_new)

        result = {
            'predictions_labels': predictions_labels,
            'n_predictions': len(predictions_labels)
        }

        print(f"=== Ensemble Voting Predictions on New Data ===")
        print(f"Number of new samples: {len(predictions_labels)}")
        print(
            f"Predicted labels distribution: {dict(zip(*np.unique(predictions_labels, return_counts=True)))}")

        if y_new is not None:
            # Evaluate predictions
            accuracy = accuracy_score(y_new, predictions_labels)
            precision_macro = precision_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            recall_macro = recall_score(
                y_new, predictions_labels, average='macro', zero_division=0)
            f1_macro = f1_score(y_new, predictions_labels,
                                average='macro', zero_division=0)

            precision_weighted = precision_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            recall_weighted = recall_score(
                y_new, predictions_labels, average='weighted', zero_division=0)
            f1_weighted = f1_score(
                y_new, predictions_labels, average='weighted', zero_division=0)

            # Per-class metrics
            precision_per_class = precision_score(
                y_new, predictions_labels, average=None, zero_division=0)
            recall_per_class = recall_score(
                y_new, predictions_labels, average=None, zero_division=0)
            f1_per_class = f1_score(
                y_new, predictions_labels, average=None, zero_division=0)

            result['evaluation'] = {
                'classification': {
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted,
                    'precision_per_class': dict(zip(self.label_encoder.classes_, precision_per_class)),
                    'recall_per_class': dict(zip(self.label_encoder.classes_, recall_per_class)),
                    'f1_per_class': dict(zip(self.label_encoder.classes_, f1_per_class))
                }
            }

            print("\n=== Evaluation Metrics on New Data ===")
            print("\nClassification Report:")
            print(classification_report(y_new, predictions_labels,
                  target_names=self.label_encoder.classes_, zero_division=0))

            if show_confusion_matrix:
                self._plot_confusion_matrix_new_data(y_new, predictions_labels)

        return result

    def evaluate(self, X_test: Optional[np.ndarray] = None, y_test: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        Evaluate the ensemble on test data.

        Args:
            X_test (np.ndarray, optional): Test features (uses model's data_split if None)
            y_test (np.ndarray, optional): Test labels (uses model's data_split if None)

        Returns:
            Dict[str, Any]: Evaluation metrics
        """
        if X_test is None or y_test is None:
            # Use the first model's test data
            X_test = self.models[0].data_split['X_test']
            y_test = self.models[0].data_split['y_test']

        predictions_labels = self.predict_labels(X_test)

        accuracy = accuracy_score(y_test, predictions_labels)
        precision_macro = precision_score(
            y_test, predictions_labels, average='macro', zero_division=0)
        recall_macro = recall_score(
            y_test, predictions_labels, average='macro', zero_division=0)
        f1_macro = f1_score(y_test, predictions_labels,
                            average='macro', zero_division=0)

        print("=== Ensemble Voting Evaluation Metrics ===")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Macro F1: {f1_macro:.4f}")

        return {
            'classification': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro
            }
        }

    def _plot_confusion_matrix_new_data(self, y_true: np.ndarray, y_pred: List[str], show_plot: bool = True) -> np.ndarray:
        """
        Compute and optionally plot the confusion matrix for new data.
        """
        cm = confusion_matrix(
            y_true, y_pred, labels=self.label_encoder.classes_)

        print("\nConfusion Matrix (Text) for New Data:")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            report = classification_report(
                y_true, y_pred, target_names=self.label_encoder.classes_, zero_division=0)

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            total = cm.sum()
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Ensemble Confusion Matrix for New Data')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Ensemble Classification Report for New Data')
            ax2.axis('off')

            plt.tight_layout()
            plt.show()

        return cm

    def plot_confusion_matrix(self, X_test: Optional[np.ndarray] = None, y_test: Optional[np.ndarray] = None, show_plot: bool = True) -> np.ndarray:
        """
        Plot confusion matrix for test data.
        """
        if X_test is None or y_test is None:
            X_test = self.models[0].data_split['X_test']
            y_test = self.models[0].data_split['y_test']

        y_pred_labels = self.predict_labels(X_test)

        cm = confusion_matrix(y_test, y_pred_labels,
                              labels=self.label_encoder.classes_)

        print("\nEnsemble Confusion Matrix (Text):")
        print("Predicted ->")
        print("Actual |", " | ".join(
            f"{label:>8}" for label in self.label_encoder.classes_))
        print("-" * (10 + 10 * len(self.label_encoder.classes_)))
        for i, true_label in enumerate(self.label_encoder.classes_):
            row = [f"{cm[i, j]:>8}" for j in range(
                len(self.label_encoder.classes_))]
            print(f"{true_label:>6} | {' | '.join(row)}")

        if show_plot:
            report = classification_report(
                y_test, y_pred_labels, target_names=self.label_encoder.classes_, zero_division=0)

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            total = cm.sum()
            annot = np.array([[f"{cm[i, j]}\n({cm[i, j]/total*100:.1f}%)"
                             for j in range(cm.shape[1])]
                              for i in range(cm.shape[0])])

            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=self.label_encoder.classes_,
                        yticklabels=self.label_encoder.classes_, ax=ax1)
            ax1.set_title('Ensemble Confusion Matrix')
            ax1.set_ylabel('True Label')
            ax1.set_xlabel('Predicted Label')

            ax2.text(0.1, 0.5, report, fontsize=10,
                     verticalalignment='center', fontfamily='monospace')
            ax2.set_title('Ensemble Classification Report')
            ax2.axis('off')

            plt.tight_layout()
            plt.show()

        return cm

    def save_model(self, filepath: str) -> None:
        """
        Save the ensemble model (note: individual models must be saved separately).
        """
        model_data = {
            'models': self.models,  # This will save the actual model instances
            'voting': self.voting,
            'weights': self.weights,
            'label_encoder': self.label_encoder
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Ensemble model saved to {filepath}")

    @staticmethod
    def load_model(filepath: str) -> 'EnsembleVotingClassifier':
        """
        Load a saved ensemble model.
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        instance = EnsembleVotingClassifier(
            model_data['models'],
            voting=model_data['voting'],
            weights=model_data['weights']
        )
        instance.label_encoder = model_data['label_encoder']
        return instance
